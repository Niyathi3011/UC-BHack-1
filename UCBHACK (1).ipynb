{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bec6b0d5-89f9-4734-9bfa-18c2efb5f6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c398c2-6531-48c8-8229-314aed74353f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4497dc06-b1bb-4144-beff-6f5399911f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in /home/ray/anaconda3/lib/python3.9/site-packages (0.6.27)\n",
      "Requirement already satisfied: openai>=0.26.4 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (0.27.8)\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (0.8.0)\n",
      "Requirement already satisfied: tiktoken in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (0.4.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (0.5.8)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (8.2.2)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.15 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (2.0.16)\n",
      "Requirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (4.5.0)\n",
      "Requirement already satisfied: urllib3<2 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (1.26.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (2023.5.0)\n",
      "Requirement already satisfied: langchain>=0.0.154 in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (0.0.203)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.9/site-packages (from llama_index) (2.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from typing-inspect==0.8.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (2.29.0)\n",
      "Requirement already satisfied: langchainplus-sdk>=0.0.9 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (0.0.10)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (1.10.8)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (3.8.4)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (2.8.4)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (1.2.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from langchain>=0.0.154->llama_index) (4.0.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from dataclasses-json->llama_index) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from dataclasses-json->llama_index) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /home/ray/anaconda3/lib/python3.9/site-packages (from openai>=0.26.4->llama_index) (4.65.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from sqlalchemy>=2.0.15->llama_index) (2.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->llama_index) (2023.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ray/anaconda3/lib/python3.9/site-packages (from tiktoken->llama_index) (2023.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.154->llama_index) (6.0.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama_index) (1.13.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.154->llama_index) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests<3,>=2->langchain>=0.0.154->llama_index) (2023.5.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json->llama_index) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daca8432-76f5-45cc-8578-b1c517c05931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import ListIndex, NotionPageReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7c5273-b67a-4434-9849-f5dd80b0747e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integration_token = os.getenv(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2868c30c-e20c-4aea-aa7e-54fd2e5abf99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token  = 'secret_i4DWTUiJ3yu1qzo05QU4MR8oaLoj3gdp22j8BDMLGDA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497d0183-9e38-4dbe-a94a-24669aa2e21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['NOTION_INTEGRATION_TOKEN'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eac89d3e-abed-4c19-909c-b1ccf6349de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "integration_token = os.getenv(\"NOTION_INTEGRATION_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dc333a0-c720-4ce9-bddd-034d8071f731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_ids = ['51522920a0934fdbb4567e7640f2e7d3','d941c28a34934793b61a509ad5c2360c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642339b4-8003-42f3-9842-1bab376077ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5375904b-d9c4-4c83-934e-6d2d8d019430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# database_id_nlp = 'd941c28a34934793b61a509ad5c2360c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30774ee4-02d5-4091-8e1a-69746fb1bf88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e45124d7-1869-43b8-aba4-786695a8d811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_dict[database_ids[0]] = \"CV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d71d146-8820-462a-a289-f780dda221af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_dict[database_ids[1]] = \"NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f33fc4b-3d3b-4881-a6b0-ebbfeef0bd30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'51522920a0934fdbb4567e7640f2e7d3': 'CV',\n",
       " 'd941c28a34934793b61a509ad5c2360c': 'NLP'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ff8a60-b920-4e84-9553-10c7ab5ea12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + token,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": \"2021-05-13\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba375357-60c4-4d58-8944-2d99e7cdee1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# my_dict['51522920a0934fdbb4567e7640f2e7d3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b335ff73-90c4-4f25-83f8-54741337afdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readDatabase(databaseId, headers):\n",
    "    readUrl = f\"https://api.notion.com/v1/databases/{databaseId}/query\"\n",
    "\n",
    "    res = requests.request(\"POST\", readUrl, headers=headers)\n",
    "    data = res.json()\n",
    "    print(res.status_code)\n",
    "    # print(res.text)\n",
    "    with open('./db.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "    research_papers_list = data['results']\n",
    "    return research_papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3501ea36-ccfb-476a-84a2-61a8d742fff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_pages_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6653428-5bf1-42c7-a7ff-0b70c95e1eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_urls(res, url_list):\n",
    "    url_list.append(res['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5ce1970-7456-47a4-af7c-c921b424d08b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_page_id(notion_url:str)->str:\n",
    "    page_id = os.path.basename(urllib.parse.urlparse(notion_url).path).split(\"-\")[-1]\n",
    "    page_id = (page_id[:8]\n",
    "               +\"-\"+page_id[8:12]\n",
    "               +\"-\"+page_id[12:16]\n",
    "               +\"-\"+page_id[16:20]\n",
    "               +\"-\"+page_id[20:])\n",
    "    return page_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d8e369-0cf1-4b27-b8f6-2a7f7b6947c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "https://www.notion.so/Conditional-Human-Sketch-Synthesis-with-Explicit-Abstraction-Control-701256f38bc34603bcaedc3b7b2ab291\n",
      "https://www.notion.so/Crowd-Powered-Photo-Enhancement-Featuring-an-Active-Learning-Based-Local-Filter-ce471cccc71245af8d9e5c7145a186a1\n",
      "https://www.notion.so/ArtFusion-Arbitrary-Style-Transfer-using-Dual-Conditional-Latent-Diffusion-Models-156a5ee04c2b4044ab05bdaef9866679\n",
      "https://www.notion.so/Seeing-the-World-through-Your-Eyes-ed3fa7f2fa0f4b8da2f1994cda972a5e\n",
      "https://www.notion.so/Seeing-the-Pose-in-the-Pixels-Learning-Pose-Aware-Representations-in-Vision-Transformers-f42b1e87d48442eba7d91d621d749cf4\n",
      "200\n",
      "https://www.notion.so/Attention-Is-All-You-Need-e3ffe835e6a74f0ab84fe9170e26aecd\n",
      "https://www.notion.so/Predictive-Biases-in-Natural-Language-Processing-Models-A-Conceptual-Framework-and-Overview-948ea63b531440d6a5feb14f38fb095c\n",
      "https://www.notion.so/ScispaCy-Fast-and-Robust-Models-for-Biomedical-Natural-Language-Processing-52998efebac8451ea5b973705822de7a\n",
      "https://www.notion.so/A-Survey-of-the-State-of-Explainable-AI-for-Natural-Language-Processing-56d795303a6f4d26973454a882ae7933\n",
      "https://www.notion.so/Feature-Extraction-and-Analysis-of-Natural-Language-Processing-for-Deep-Learning-English-Language-ff7e59412914487db7523e57eb439d93\n"
     ]
    }
   ],
   "source": [
    "for db_id in database_ids:\n",
    "    category = my_dict[db_id]\n",
    "    res = readDatabase(db_id, headers)\n",
    "    page_ids = []\n",
    "    for l in res:\n",
    "        print(l['url'])\n",
    "        page_ids.append(get_page_id(l['url']))\n",
    "        \n",
    "    database_pages_dict[category] = page_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ac7799d-d056-49ee-ac5b-96f6da42bf12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CV': ['701256f3-8bc3-4603-bcae-dc3b7b2ab291',\n",
       "  'ce471ccc-c712-45af-8d9e-5c7145a186a1',\n",
       "  '156a5ee0-4c2b-4044-ab05-bdaef9866679',\n",
       "  'ed3fa7f2-fa0f-4b8d-a2f1-994cda972a5e',\n",
       "  'f42b1e87-d484-42eb-a7d9-1d621d749cf4'],\n",
       " 'NLP': ['e3ffe835-e6a7-4f0a-b84f-e9170e26aecd',\n",
       "  '948ea63b-5314-40d6-a5fe-b14f38fb095c',\n",
       "  '52998efe-bac8-451e-a5b9-73705822de7a',\n",
       "  '56d79530-3a6f-4d26-9734-54a882ae7933',\n",
       "  'ff7e5941-2914-487d-b752-3e57eb439d93']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_pages_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1032080-90d0-4d90-a126-bbbe8c48b30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(database_pages_dict['CV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "423e67e7-329a-4d8c-8fd4-07a8933d71a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV\n",
      "['701256f3-8bc3-4603-bcae-dc3b7b2ab291', 'ce471ccc-c712-45af-8d9e-5c7145a186a1', '156a5ee0-4c2b-4044-ab05-bdaef9866679', 'ed3fa7f2-fa0f-4b8d-a2f1-994cda972a5e', 'f42b1e87-d484-42eb-a7d9-1d621d749cf4']\n",
      "NLP\n",
      "['e3ffe835-e6a7-4f0a-b84f-e9170e26aecd', '948ea63b-5314-40d6-a5fe-b14f38fb095c', '52998efe-bac8-451e-a5b9-73705822de7a', '56d79530-3a6f-4d26-9734-54a882ae7933', 'ff7e5941-2914-487d-b752-3e57eb439d93']\n"
     ]
    }
   ],
   "source": [
    "for key, values in database_pages_dict.items():\n",
    "    print(key)\n",
    "    print(values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "acfda4bd-d4e6-4b0b-93b3-eb32a4757808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8739f14a-5e8f-4a58-9eb1-8323557651fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "documents_with_extra_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1db2b60-04d8-4af9-a5e5-7579639bb42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for key, values in database_pages_dict.items():\n",
    "    page_documents = NotionPageReader(integration_token=integration_token).load_data(page_ids=values)\n",
    "    for doc in page_documents:\n",
    "        # print(doc.text)\n",
    "        document = Document(\n",
    "            doc.text,\n",
    "            extra_info={ \n",
    "                'catgeory':key,\n",
    "                'filename':key\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    documents_with_extra_info.append(document)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d789c2d-40ca-41e6-b4ae-8f3919f67d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index import TreeIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "515b2f14-4cdd-4d53-95e2-540f60e6f324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53ebd703-140c-4f00-9d99-050eddee1238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcdc394b-9a67-4dfb-8476-1ad0cba07293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(text='Dominick Reilly\\nUNC Charlotte\\nAman Chadha∗\\nStanford University, Amazon Alexa AI\\n\\ndreilly1@charlotte.edu\\n\\nSrijan Das\\nUNC Charlotte\\nAbstract\\nHuman perception of surroundings is often guided by the various poses present\\nwithin the environment. Many computer vision tasks, such as human action\\nrecognition and robot imitation learning, rely on pose-based entities like human\\nskeletons or robotic arms. However, conventional Vision Transformer (ViT) models\\nuniformly process all patches, neglecting valuable pose priors in input videos.\\nWe argue that incorporating poses into RGB data is advantageous for learning\\nfine-grained and viewpoint-agnostic representations. Consequently, we introduce\\ntwo strategies for learning pose-aware representations in ViTs. The first method,\\ncalled Pose-aware Attention Block (PAAB), is a plug-and-play ViT block that\\nperforms localized attention on pose regions within videos. The second method,\\ndubbed Pose-Aware Auxiliary Task (PAAT), presents an auxiliary pose prediction\\ntask optimized jointly with the primary ViT task. Although their functionalities\\ndiffer, both methods succeed in learning pose-aware representations, enhancing\\nperformance in multiple diverse downstream tasks. Our experiments, conducted\\nacross seven datasets, reveal the efficacy of both pose-aware methods on three video\\nanalysis tasks, with PAAT holding a slight edge over PAAB. Both PAAT and PAAB\\nsurpass their respective backbone Transformers by up to 9.8% in real-world action\\nrecognition and 21.8% in multi-view robotic video alignment. Code is available at\\n\\nhttps://github.com/dominickrei/PoseAwareVT\\n.\\n1 Introduction\\nDespite the recent advancements in AI, video understanding remains a formidable task within the\\nfield of computer vision. Transformers [67, 18, 64, 12, 73, 29], referenced in many recent studies,\\nhave proven their dominance across various domains, including vision, thanks to their effective use\\nof self-attention and feed-forward layers. When these transformers are applied to spatio-temporal\\ndomains, video transformers [6, 3, 41, 45, 20] have demonstrated significant potential. However,\\none limitation is that these video transformers treat all input patches uniformly without accounting\\nfor any priors, when applying operations. This holistic approach is effective for web-sourced\\nvideos [32, 7, 60, 35], where prominent motion patterns are typically centered within the image\\nframes. However, the effectiveness of these transformers tends to fall short when employed on daily\\nliving videos [38, 53, 15, 68, 69] containing subtle motion, non choreographed scenes and varying\\ncamera viewpoints. Understanding these videos require learning fine-grained and camera viewpoint\\nagnostic representations.\\nDaily living videos often contain entities defined by their poses. However, traditional ViTs tend to\\noverlook these pose-based entities during video processing. The effectiveness of 3D pose information\\nis well established in video analysis [56, 42, 19, 39, 33]. Nevertheless, these pose-based methods are\\n∗This work is unrelated to the position at Amazon.\\nPreprint. Under review.\\narXiv:2306.09331v1 [\\ncs.CV\\n] 15 Jun 2023\\nsomewhat limited in their ability to model the appearance of a scene. To address this shortcoming,\\nsome studies [5, 4, 16] have attempted to combine 3D poses with RGB. Yet, we posit that acquiring\\n3D poses can be challenging, especially in the absence of a depth sensor, due to the relative inaccuracy\\nof available algorithms and their high computational costs.\\nQuery patch Pose keypoints\\n⋯\\n𝑇\\nPose-aware Learning (ours)\\n⋯\\n𝑇\\nTraditional Approach\\nAttended patches\\nFigure 1: The goal of our proposed framework. Our aim is to learn pose-aware features for ViTs,\\nwhile maintaining the whole-scene knowledge learned in the traditional approach to training ViTs.\\nConsequently, in this paper, we propose learning pose-aware video representations within ViTs by\\nutilizing the capabilities of 2D pose keypoints (see Figure 1). These keypoints, known for their\\nprecision, can be easily extracted using readily available pose estimation algorithms [8]. To enable the\\nlearning of pose-aware representation, we propose two approaches: the first involves the formulation\\nof a network architecture explicitly tailored for localized attention on pose patches and the second\\nintroduces an auxiliary pose prediction task that is jointly optimized with the primary task. These\\napproaches has culminated in the introduction of two novel methods: Pose-aware Attention Block\\n(PAAB) and Pose-aware Auxiliary Task (PAAT). Both PAAB and PAAT can be plugged into an\\nexisting ViT, with an auxiliary loss being added for the latter. Despite their differing functionalities,\\nboth PAAB and PAAT learn to disentangle between pose and non-pose patches within a video. Our\\nanalysis leads us to the striking observation that the learned pose-aware representations are not merely\\na result of the pose-guided sparsity of the ViT’s attention weights, but are instead achieved through\\nthe feed-forward layers within the ViT. The efficacy of PAAB and PAAT is validated across three\\ndownstream tasks encompassing three datasets for action recognition, four for multi-view robotic\\nvideo alignment, and a cross-data evaluation for video retrieval. Both PAAB and PAAT significantly\\noutperform the baseline Transformer across all datasets, achieving state-of-the-art results relative to\\nrepresentative baselines.\\n2 Background: Attention in Video Transformers\\nPose-aware representation learning is based on incorporating pose information into the training\\nprocess of existing ViTs. As such, we briefly review how attention is performed in ViT’s for video\\ndata. Consider a video input of size τ × H × W × 3, where τ frames have a spatial resolution\\nof H × W and three color channels. Most video transformers [50] extract disjoint patches from\\nthe video, resulting in an input sequence of ST tokens, with S being the spatial resolution and T\\nbeing the temporal resolution. Each of these tokens are then projected to R\\nD via a linear layer.\\nSubsequently, two learnable position embeddings are added to each token to encode spatial and\\ntemporal position information, respectively. Furthermore, a class token is added to the input sequence\\nprior to its processing by the transformer to enable the classification of the entire video. Note that\\nthis class token can be used for performing other downstream tasks as well.\\nSimilarly to the standard transformer, the input sequence is transformed into key, query, and value\\nmatrices denoted as K ∈ R\\nST ×D, Q ∈ R\\nST ×D, and V ∈ R\\nST ×D, respectively. Conventional\\nself-attention [67] computes the pairwise similarities between all combinations of tokens in the input\\nsequence. In the realm of video transformers this is known as joint space-time attention, as similarity\\nis computed between all tokens, regardless of their spatial or temporal position as:\\nα\\njoint\\nst =\\nexp(QstK⊤)\\nP\\ns\\n′t\\n′ exp(QstK⊤\\ns\\n′t\\n′ )\\n(1)\\nwhere, Qst, Ks\\n′t\\n′ are the D-dimensional query and key vector for the token at spatial and temporal\\nposition s, t respectively. However, this approach for computing attention in video transformers\\nis expensive due to the quadratic complexity of self-attention and the large size of video data. To\\n2\\naddress this, factorized self-attention has been proposed in [6]. This mechanism is termed divided\\nspace-time attention and is achieved by applying temporal-attention followed by spatial-attention as:\\nα\\ntime\\nst =\\nexp(QstK⊤\\ns:\\n)\\nP\\nt\\n′ exp(QstK⊤\\nst′ )\\n; α\\nspatial\\nst =\\nexp(QstK⊤\\n:t\\n)\\nP\\ns\\n′ exp(QstK⊤\\ns\\n′t\\n)\\n(2)\\nwhere, K:t indicates a slice of K across the t\\nth frame (i.e., the keys for all spatial tokens in frame t).\\nThe remaining operations within video transformers follow the same principles as standard vision\\ntransformers [18].\\n3 Pose-Aware Representation Learning\\nThis section presents our approaches to learning pose-aware representations by utilizing existing\\nVision Transformer (ViT) architectures for video understanding. Towards this objective, we propose\\ntwo distinct approaches. The first approach entails introducing architectural changes through the\\nincorporation of a novel PAAB that integrates knowledge of poses into the ViT representation. In\\ncontrast, the second approach involves the use of PAAT, a multi-tasking objective function to reinforce\\nthe ViT’s focus on poses, facilitating the learning of pose-aware representations.\\n3.1 Pose map instantiations\\nDue to the nature of our methods, we must have a correspondence between the video patches and the\\npose configurations of objects within the video. We achieve this through the construction of two pose\\nmaps: P\\n2D and P\\n3D, the details of which are described below.\\nThe pose configuration of an object in a video is typically characterized by its 2D pose, which is\\nrepresented by a set of 2D coordinates (known as keypoints) that provides the specific locations of\\nrelevant parts of the object. For instance, in human action videos these keypoints correspond to the\\nlocations of various human joints (hand, foot, etc) in each video frame. The localization of these\\nkeypoints can be achieved by exploiting pose estimation algorithms such as [8, 21], which are high\\nprecision algorithms and are commonly used in video analysis. After extracting these keypoints, we\\nobtain a set K that denotes the coordinates of the keypoints within each frame:\\nK = {(t, k, x, y)} : 1 ≤ t ≤ τ, 1 ≤ k ≤ K (3)\\nwhere K is the number of keypoints. We then define a pose map, P, of resolution τ × K × H × W\\nas:\\nPtkxy =\\n\\n1 if (t, k, x, y) ∈ K\\n0 otherwise (4)\\nThus, Ptkxy = 1 if the k\\nth keypoint is present at the (x, y)\\nth pixel in the t\\nth video frame. To\\nalign with ViT inputs, P is decomposed into ST disjoint patches, transforming P into a K ×\\nST × p × p dimensional binary matrix, where p is the patch size. Each patch is transformed as\\nP\\n2D\\ni = MaxPool(Pi), i.e., if the patch Pi contains one or more keypoints, it is set to one, else it\\nremains zero. Thus, P\\n2D is a ST dimensional binary vector indicating the video patches that contain\\nany keypoints.\\nWe also compute a 3D instantiation of the pose map, denoted as P\\n3D. In contrast to P\\n2D, P\\n3D\\nik = 1\\nif the k\\nth keypoint lies in the patch Pi\\n, otherwise it is zero. Thus, P\\n3D is a ST × K dimensional\\nbinary vector indicating video patches that contain a specific keypoint.\\n3.2 Pose-Aware Attention Block (PAAB)\\nThe Pose-Aware Attention Block (PAAB) is a plug-in module that can be inserted into existing ViT\\narchitectures to induce learning of pose-aware representations. When inserted into a ViT, PAAB will\\nprocess tokens from the previous layer and return a set of tokens enriched with pose information,\\nthese enriched tokens can be propagated as usual through the rest of the ViT. PAAB accomplishes\\nthis through a pose-aware self-attention mechanism, restricting interactions to tokens representing\\nhuman keypoints, i.e., pose tokens. Essentially, PAAB functions as a local attention that modulates\\nthe pose token representation based on its interaction with other pose tokens within a video. As\\n3\\nInput video frames\\nJoint Pose-aware Self-attention\\nTime\\nSpatial Pose-aware Self-attention\\nPose patches Pose keypoints Query Patch Attention\\n(a) Visual of pose-aware attention schemes.\\nLayer Norm\\nJoint Pose-aware\\nSelf-attention\\nLayer Norm\\nMLP\\nJoint Pose-aware\\nAttention Block\\nPrevious Layer\\nLayer Norm\\nSpatial Pose-aware\\nSelf-attention\\nLayer Norm\\nMLP\\nSpatial Pose-aware\\nAttention Block\\nPrevious Layer\\nLayer Norm\\nTemporal Selfattention\\nLayer Norm\\nSpatial Pose-aware\\nSelf-attention\\nLayer Norm\\nMLP\\nSpatio-Temporal Pose-aware\\nAttention Block\\npass only\\nthe pose\\ntokens\\nPrevious Layer\\n(b) Variants of Pose-aware Transformer Block. Dashed\\nlines indicated residuals.\\nFigure 2: Overview of Pose-Aware Attention Block. PAAB takes tokens processed by a ViT as input\\nand applies a pose-aware attention to them. The attention is applied jointly (over pose tokens from\\nall frames), spatially (over pose tokens in a single frame), or spatio-temporally. In spatio-temporal\\nattention, traditional temporal attention is applied followed by pose aware spatial attention.\\nshown in Figure 2b, PAAB comes in three variants, namely, joint (Joint PA-STA), spatial (PA-SA),\\nand spatio-temporal (Factorized PA-STA) pose-aware self-attention, each differing based on how\\npose tokens interact with each other.\\nThe joint variant of PAAB (see Figure 2a) extends the joint space-time attention in equation 1 by\\nleveraging the 2D pose map P\\n2D as\\nα\\nPA−joint\\nst =\\n( exp(QstK⊤⊙P2D\\nP\\n)\\ns′t\\n′ exp(QstK⊤\\ns′t\\n′⊙P2D\\ns′t\\n′\\n)\\nif P\\n2D\\nst = 1\\n0 if P\\n2D\\nst = 0\\n(5)\\nwhere ⊙ is the Hadamard product. Similarly, a spatial variant of PAAB learns attention weights\\nα\\nPA−spatial\\nst for the token at (s, t) as,\\nα\\nPA−spatial\\nst =\\n( exp(QstK⊤\\n:t ⊙P2D\\n:t P\\n)\\ns′ exp(QstK⊤\\ns′t\\n⊙P2D\\ns′t\\n)\\nif P\\n2D\\nst = 1\\n0 if P\\n2D\\nst = 0\\n(6)\\nUsing P\\n2D, the spatial attention in PAAB allows interaction amongst pose tokens in a single frame\\n(see Figure 2a). The spatio-temporal variant of PAAB entails temporal attention αtime, applied to all\\ntokens as described by equation 2. This is then followed by a pose-aware spatial attention αPA−spatial\\n,\\nas depicted by equation 6. Note that the aforementioned conditional pose-aware attention weights\\nare implemented in practice through a differential approximation, whereby P\\n2D = ∞(P\\n2D − 1)\\nis transformed, followed by adding (instead of Hadamard product) with QstK⊤, as performed in\\nequation 5 and 6. This equates to masking out the attention values of all non-pose tokens, similarly to\\nhow the decoder in NLP transformers masks out unseen tokens [67].\\n3.3 Pose-Aware Auxiliary Task (PAAT)\\nIn contrast to PAAB, which utilizes a local attention mechanism to facilitate pose-aware representation\\nlearning, PAAT attempts at achieving the same through the introduction of an auxiliary task that\\nis jointly optimized alongside the primary ViT task. When inserted into a ViT, PAAT’s goal is to\\nclassify the specific keypoints present in each patch using the intermediate token representations\\nobtained from the ViT layer preceeding PAAT. In other words, its goal is to predict the 3D pose map\\nP\\n3D. This task can be realized as a multi-label multi-class classification problem, as each patch can\\ncontain multiple keypoints (illustrated in Figure 3a).\\nGiven a set of tokens from the layer preceeding PAAT, zl−1 ∈ R\\nST ×D, PAAT predicts the 3D pose\\nmap introduced in section 3.1. Recall that each D-dimensional token in zl−1 corresponds to the\\nlatent representation of a video patch from the (l − 1)th ViT layer. As depicted in Figure 3b, PAAT is\\nformulated as a patch-keypoint classifier that is composed of two linear layers defined by the weights\\nW1 ∈ R\\nD×De and W2 ∈ R\\nDe×K, where De is the bottleneck dimension and De ≤ D. The 3D\\npose map predicted by PAAT inserted at l\\nth layer of a ViT is given by:\\n4\\nR-Hand keypoint\\nHead keypoint\\nL-Hand keypoint 0\\n1\\nPose patches\\nKeypoint not contained in patch\\nKeypoint contained in patch\\nInput video frames with 3D pose map (𝒫\\n3𝐷) visualized\\n(a) 3D Pose maps generated from the video frames\\nLinear Projection\\nof Patches\\nVision\\nTransformer\\nLayers\\nVision\\nTransformer\\nLayers\\nPose-aware Video\\nRepresentation\\nPatch-keypoint\\nclassifier Linear Linear\\nVideo Frames\\nPredicted 3D pose map (𝒫\\u0de03𝐷)\\nTraining Only\\nTraining and\\nInference\\nTrue 3D pose map (𝒫\\n3𝐷)\\nℒ𝑃𝐴𝐴𝑇(𝒫\\n3𝐷, 𝒫\\u0de03𝐷)\\n(b) Pipeline\\nFigure 3: Overview of Pose-Aware Auxiliary Task. Given the 3D pose map, PAAT learns to predict\\nthe specific keypoint present within each video patch. This task is learned at train time via the\\npatch-keypoint classifier, and can be discared at inference time.\\nPˆ3D = σ((zl−1W1)W2) (7)\\nwhere σ is the sigmoid activation. For brevity, we omit the bias terms. PAAT’s loss is computed as\\nthe binary cross-entropy (BCE) between P\\n3D and Pˆ3D. During training, PAAT is optimized jointly\\nwith the primary task, such as classification or video alignment, with a loss Lprimary. Consequently,\\nPAAT loss, LPAAT, and model’s total loss, Ltotal, is defined as,\\nLPAAT = BCE(P\\n3D,Pˆ3D); Ltotal = λLP AAT + Lprimary (8)\\nwhere λ is a scaling factor that controls the influence of PAAT on the model training. At training, the\\ngradient updates with ∂zl−1\\n∂LPAAT\\nat the (l − 1)th layer force the ViT to learn pose-aware representations\\nthat discriminate between pose and non-pose tokens. This enables the remaining transformer layers\\nto encode pose-aware representations. At inference, the patch-keypoint classifier is discarded and the\\nViT can be used with no remnants of PAAT.\\n4 Experiments\\nWe evaluate the effectiveness of the proposed pose-aware learning methods on three diverse computer\\nvision tasks: (i) action recognition, (ii) multi-view robotic video alignment, and (iii) video retrieval.\\nWe provide an experimental analysis that demonstrates the effectiveness of our methods across these\\ndiverse tasks. We also perform an extensive diagnosis on our model and discuss the intriguing\\nproperties we observe.\\n4.1 Datasets & Evaluation protocols\\nAction recognition is a popular video analysis task whose goal is to learn to predict an action label\\ngiven a trimmed video. For this task we evaluate our methods on three popular Activities of Daily\\nLiving (ADL) datasets: Toyota-Smarthome [15] (Smarthome, SH), NTU-RGB+D [53] (NTU), and\\nNorthwestern-UCLA Multiview activity 3D Dataset [69] (NUCLA). For the Toyota-Smarthome\\ndataset, we adhere to the cross-subject (CS) and cross-view (CV1, CV2) protocols, gauging performance using the mean class-accuracy (mCA) metric. When assessing the NTU-RGB+D dataset,\\nwe follow the cross-view-subject (CVS) protocols proposed in [66], as they better represent the\\ncross-view challenge. As for the NUCLA dataset, we report the accuracy on cross-subject (CS),\\ncross-view (CV3), and the average across all the cross-view protocols. For the extraction of 2D pose\\nkeypoints, we employed LCRNet [48], Randomized Decision Forest [57], and OpenPose [8] for\\nSmarthome, NTU and NUCLA datasets respectively. Note that all our ablation studies are conducted\\non the action classification task.\\nMulti-view robotic video alignment is a task to learn a frame-to-frame mapping between video\\npairs acquired from different camera viewpoints. Such tasks are able to facilitate robot imitation\\nlearning from third-person viewpoints [55]. For this task we use the Minecraft (MC), Pick, Can,\\nand Lift datasets. These datasets are obtained from a range of environments: Minecraft from video\\ngame whereas Pick, Can, and Lift from robotics simulators (PyBullet[14], Robomimic[44]).The pixel\\npositions of the robotic arms, regarded as the poses, are obtained from the simulators. This task is\\n5\\nTable 1: Ablations on PAAB and PAAT. We perform ablations on the following: (a) position of\\nPAAB and PAAT, (b) variants of PAAB, (c) number of PAABs to insert, and (d) variants of PAAT.\\n(a) PAAB performs best when inserted near the end of the model, PAAT performs best when inserted\\nat the beginning. Inserting the PAAB or PAAT at multiple positions is not necessary.\\nDataset Baseline PAAB Position PAAT Position\\n1 6 12 1,6 1,12 1 6 12 1,6 1,12\\nSH (CS) 68.4 67.1 67.7 71.4 67.1 68.6 72.5 70.9 69.9 70.7 70.2\\nSH (CV1) 50.0 50.5 52.4 54.9 51.5 50.5 54.8 52.2 47.6 49.7 51.5\\nNTU (CVS1) 83.5 85.0 85.7 85.2 85.5 85.2 85.4 85.2 84.5 84.6 84.2\\n(b) PA-SA is sufficient despite having\\nthe least additional parameters.\\nDataset PA-SA Factorized Joint\\nPA-STA PA-STA\\nSH (CS) 71.4 69.9 69.8\\nSH (CV1) 54.9 50.2 52.0\\nNTU (CVS1) 85.2 85.4 85.8\\nNTU (CVS3) 51.6 51.3 49.2\\n(c) Inserting 1 PAAB after layer 12 is the\\nmost consistent across datasets.\\nDataset # PAABs at layer 12\\n1 2 3 4\\nSH (CS) 71.4 67.1 69.0 66.7\\nSH (CV1) 54.9 51.3 51.18 48.4\\nNTU (CVS1) 85.2 84.7 85.0 85.6\\n(d) PAAT performs best when\\npredicting P\\n3D over P\\n2D\\nDataset Variant Accuracy\\nSH (CS) P\\n2D 71.1\\nP\\n3D 72.5\\nNTU (CVS1) P\\n2D 84.0\\nP\\n3D 85.4\\nevaluated by an alignment error metric introduced in [51]. Sample frames from these datasets are\\nprovided in Fig. 6.\\nVideo Retrieval is a nearest-neighbour retrieval task performed on learned features without any\\nfurther training. For evaluation, we report the Recall at k (R@k), meaning, if the top k nearest\\nneighbours contains one video of the same class, the retrieval was successful.\\n4.2 Implementation\\nIn our default implementations of PAAB, we use the spatial attention variant (PA-SA) inserted after\\nthe 12th layer of a backbone ViT. For PAAT, we insert it after the 1\\nst layer of the backbone. For the\\nimplementation of PAAT, we use a bottleneck dimension (De) of 256 for the patch-keypoint classifier\\nand a loss scale (λ) of 1.6. In our experiments, we use a TimeSformer [6] backbone for the task of\\naction recognition and video retrieval, while a DeiT [64] backbone is utilized for video alignment.\\nAll other training and dataset specific details are provided in the Appendix.\\n4.3 Ablation studies\\nWhere should we insert PAAB and PAAT? In Table 1a, we investigate the optimal insertion point\\nof PAAB and PAAT. We initially examine the performance impact of incorporating a single PAAB\\nor PAAT after specific ViT layers. Subsequently, we assess the implications of integrating multiple\\nPAABs or PAATs at varying ViT layers. Interestingly, our findings suggest a complementary dynamic\\nbetween PAAB and PAAT. PAAB exhibits superior performance when positioned closer to the ViT’s\\nclassification head, while PAAT performs better when inserted at the initial layer. This shows that the\\nauxiliary task is beneficial for improving the primary task, namely action recognition, when operating\\non low-level token representations that have not yet been contextualized [1]. In contrast, attention\\nblocks like PAAB are most effective when working with high-level token representations, which have\\nbeen extensively contextualized and optimized for the primary task.\\nWhich variant of PAAB and PAAT should be used? Here, we explore the different variants of\\nPAAB and PAAT. Table 1b presents the classification results of different PAAB variants. These\\nattention variants are arranged from left to right, corresponding to the number of extra parameters\\nthey introduce. Across the Smarthome and NTU datasets, PA-SA consistently exhibits superior\\nperformance, while the other two variants tend to result in a significant decrease in performance (for\\ninstance, a reduction of 2.81% on Smarthome (CS) when transitioning from PA-SA to Joint PA-STA).\\nIn general, despite having the lowest number of added parameters, PA-SA proves to be sufficient for\\nlearning pose-aware representations. In Table 1d, we analyze the implications of training PAAT with\\nan alternate auxiliary task. This task involves predicting the presence or absence of a keypoint in\\neach patch, rather than the specific keypoint located in each patch. Essentially, the task’s objective\\nis to predict the 2D pose map instantiation, P\\n2D. We ascertain that the patch-keypoint prediction\\n6\\ntask (predicting P\\n3D), proves more effective, underlining the significance of incorporating human\\nanatomy knowledge into the learned video representation.\\nHow many PAAB’s should you use? In Table 1c, we examine the optimal number of consecutive\\nPAABs to be incorporated into the ViT. Our findings suggest that a single PAAB is sufficient, and the\\nmodel’s performance tends to decline with an increased number of blocks. This outcome is attributed\\nto the fact that incorporating additional PAABs leads to a loss of the valuable contextual information\\nfrom non-pose tokens, which are often crucial for action recognition.\\nTable 2: Results of training our models with and\\nwithout random 2D and 3D pose maps.\\nDataset Method Random pose\\nmap\\nAccuracy (%)\\nSH (CS)\\nPAAB ✓ 70.1\\n✗ 71.4\\nPAAT ✓ 69.5\\n✗ 72.5\\nNTU60 (CVS1)\\nPAAB ✓ 85.0\\n✗ 85.8\\nPAAT ✓ 84.8\\n✗ 85.4\\n0 20 40 80 160\\nNoise Level\\n70\\n72\\nmCA (%)\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\nFigure 4: Model performance degrades\\nas we lose more of the pose information.\\n4.4 Do poses really help?\\nTo answer this question, we perform two experiments to evaluate the importance of poses. In our\\nfirst experiment, we randomly activate values in the pose maps, P\\n2D and P\\n3D, irrespective of the\\nactual presence or absence of pose keypoints in a patch. Our results, presented in Table 2, show\\nthat our methods deliver superior performance when utilizing accurate pose maps informed by pose\\nestimation as opposed to random pose maps. In the second experiment, we introduce varying levels\\nof noise to the pose keypoints before computing P\\n2D and P\\n3D. More specifically, we set a noise\\nlevel ϵ ≥ 0 and add a randomly generated integer between 0 and ϵ to the 2D coordinates of each pose\\nkeypoint in K. We then generate the pose maps as usual and train our models. The results of this\\nexperiment, conducted on the Smarthome CS protocol, are presented in Figure 4. As the reliability\\nof the poses decreases, the accuracy of the model with PAAT quickly declines towards the baseline.\\nOn the other hand, while the accuracy of the model with PAAB also drops, it stabilizes at around\\n70%. These experiments show the crucial role of poses in video understanding and demonstrate the\\nrobustness of PAAB and PAAT to considerable noise in pose information.\\nTable 3: TimeSformer + our pose-aware methods, compared to the SOTA models on Toyota-Smarthome. Modality indicates the modalities required at inference time.\\nMethod Modality Accuracy (%)\\nRGB Pose CS CV1 CV2\\nI3D [10] ✓ ✗ 53.4 34.9 45.1\\nVPN [16] ✓ 3D 60.8 43.8 53.5\\nAssembleNet++ [49] ✓ ✗ 63.6 - -\\nVPN++ [17] ✓ 3D 71.0 - 58.1\\nMMNet [72] ✓ 2D 70.1 37.4 46.6\\nVideo Swin [41] ✓ ✗ 69.8 36.6 48.6\\nMotionFormer [45] ✓ ✗ 65.8 45.2 51.0\\nTimeSformer [6] ✓ ✗ 68.4 50.0 60.6\\nPAAB (ours) ✓ 2D 71.4 54.9 61.8\\nPAAT (ours) ✓ ✗ 72.5 54.8 62.2\\n1 5 10 20\\nk\\n0.8\\n0.9\\n1.0\\nAccuracy\\nTimeSformer (Baseline)\\nPAAB (ours)\\nPAAT (ours)\\nFigure 5: NTU (CS) pre-trained PAAT &\\nPAAB (with TimeSformer backbone) for\\nk-NN video retrieval on NUCLA. Notably,\\nPAAT performs better at k = 1, 5, 10\\n4.5 Results\\nIn this section, we present the performance of our models across three downstream tasks, and present\\na state-of-the-art comparison with the representative baselines.\\nAction recognition. We compare the performance of PAAB and PAAT to various state-of-the-art\\n(SOTA) methods on the Smarthome, NTU, and NUCLA datasets in Table 3 and Table 4 (a-b). Our\\nresults reveal that PAAB and PAAT set a new benchmark on the Smarthome dataset which presents\\nthe challenges of real-world scenarios, including challenges in pose estimation. For a fair comparison\\nwith our models, we have primarily included models that leverage the RGB modality for the NTU\\n7\\nTable 4: State-of-the-art comparison on NTU-RGB+D and NUCLA.\\n(a) TimeSformer + our pose-aware methods on NTU.\\nMethod Modality Accuracy (%)\\nRGB Pose CVS1 CVS2 CVS3\\nST-GCN[25] ✗ 3D 74.8 59.8 31.4\\n3D ResNet-50 [30] ✓ ✗ 83.9 67.9 42.9\\nS3D [70] ✓ ✗ 84.1 66.4 40.1\\nVideo Swin [41] ✓ ✗ 86.9 72.7 51.4\\nMotionFormer [45] ✓ ✗ 85.3 72.2 51.3\\nTimeSformer [6] ✓ ✗ 83.5 72.6 50.3\\nPAAB (ours) ✓ 2D 85.8 73.1 51.6\\nPAAT (ours) ✓ ✗ 85.4 73.1 51.8\\n(b) TimeSformer+our pose-aware methods on NUCLA.\\nMethod Modality Accuracy (%)\\nRGB Pose CS CV3 Avg\\nGlimpse Cloud [5] ✓ ✗ - 90.1 87.6\\nVPN [16] ✓ 3D - 93.5 -\\nVPN++ [17] ✓ 3D - 93.5 -\\nMMNet ✓ 3D - 93.7 88.7\\nVideo Swin [41] ✓ ✗ 90.7 89.6 84.3\\nMotionFormer [45] ✓ ✗ 90.2 89.4 88.4\\nTimeSformer [6] ✓ ✗ 90.7 91.8 90.5\\nPAAB (ours) ✓ 2D 93.4 92.9 91.3\\nPAAT (ours) ✓ ✗ 95.4 92.7 90.8\\nand NUCLA datasets. This is despite the popularity of pure-pose based methods in these datasets, as\\nthey are more representative to our evaluation scenario. The superior performance of our models,\\nexhaustively evaluated on cross-view protocols, underlines the view- agnostic representation learned\\nby PAAB and PAAT through the use of 2D poses. Interestingly, despite solely relying on RGB, our\\nmodels exhibit competitive results when compared with methods employing both RGB and 3D poses.\\nThis shows the capability of video transformers with PAAB or PAAT to capture pose-aware features.\\nIn addition, our models are compared with prominent video transformer models [41, 45, 6]. We\\nfind that either PAAB or PAAT, when employed with TimeSformer, surpasses the performance of\\nstate-of-the-art video transformers (except on CVS1 of NTU), boasting an absolute margin of up to\\n18.3%.\\nMulti-view robotic video alignment. Table 5 illustrates the performance of our models on the\\nPick, MC, Can, and Lift datasets. We present the alignment error (lower is better) for each method.\\nNote that PAAB and PAAT are implemented in the DeiT encoder [64] which is trained with TCN\\nlosses [52]. We find that both PAAB and PAAT improves the baseline DeiT [64] by 21.8% on the\\nMC dataset, which is notable as it contains the largest viewpoint variation of all the datasets. While\\nour models deliver superior results compared to most of the SOTA methods, they fall slightly short of\\n3DTRL [55] on the Pick dataset.\\nPick MC\\nCan Lift\\nFigure 6: Example video alignment frames\\nfrom two different viewpoints.\\nTable 5: DeiT + TCN + our pose-aware methods\\non multi-view robotic video alignment. Metric is\\nalignment error.\\nMethod Backbone Pick MC Can Lift\\nTCN [52] CNN 0.273 0.286 - -\\nDisentangle [54] CNN 0.155 0.233 - -\\n3D TRL [55] ViT 0.116 0.202 0.060 0.081\\nDeiT [64]+TCN ViT 0.216 0.292 0.065 0.095\\nPAAB (ours) ViT 0.165 0.151 0.053 0.075\\nPAAT (ours) ViT 0.159 0.149 0.059 0.074\\nVideo retrieval. We demonstrate the generalizability of our models, by presenting the performance\\nof our NTU pre-trained models for video retrieval on NUCLA, as illustrated in Figure 5. However,\\nPAAB’s performance falls short in this context, while PAAT outperforms all. This shows the\\ngeneralizability power of PAAT, attributed to its joint optimization strategy.\\n4.6 Feature & Attention Analysis\\nIn this section, we explore the feature space and attention distributions learned by our models on a\\nsubset of the Smarthome. In Figure 7a, we compute the average feature distance between the pose\\nand non pose tokens in the feature space. Both PAAB and PAAT learn to better disentangle the pose\\nand the non pose token representation compared to the baseline TimeSformer, with PAAT achieving\\nsuperior feature separability due to its keypoint-specific prediction task. Figure 7b further confirms\\nthis, where PAAT exhibits better separability of individual pose features, unlike PAAB. In addition to\\nanalyzing the feature space, we also explore the attention distributions learned by our methods. In\\nFigure 7c we report the percentage of tokens across various attention value bins at different layers.\\n8\\n1 6 12\\nTransformer Layer\\n10\\n20\\n30\\nFeature Distance\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\n(a) Average pose and non-pose token feature distance.\\n1 6 12\\nTransformer Layer\\n0\\n2\\n4\\nFeature Distance\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\n(b) Average feature distance between pose tokens.\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\nAttention Bin Value\\n0\\n5\\n10\\n15\\n20\\n% Tokens\\nLayer 1\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\nAttention Bin Value\\n0\\n5\\n10 Layer 6\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0\\nAttention Bin Value\\n0\\n5\\n10\\n15 Layer 12\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\n(c) Attention distributions of layers 1, 6, and 12.\\nFigure 7: Feature & Attention Analysis. From (a) we see our methods learn to disentangle pose and\\nnon-pose tokens in the feature space. (b) shows that PAAT learns more separable pose tokens. (c)\\nshows that interestingly, our methods maintain similar attention distributions to the baseline.\\nInterestingly, the attention distributions of our models align closely with the baseline, implying that\\nour methods primarily leverage the feed-forward layers to learn pose-aware representations.\\n5 Related Works\\nIn recent years, vision transformers [18, 40, 64, 73, 29, 12] have overtaken CNNs [31, 11, 63] in\\nperformance across numerous image-based tasks [18, 9, 61]. Similarly, video transformers [6, 41,\\n3, 45, 20, 36] have had a comparable effect on 3DCNNs [22, 37, 10, 65] and two-stream CNNs for\\nvideo-based tasks [59, 23, 24]. While these video transformers are tailored for analyzing web-based\\nvideos [32, 60, 35, 27], emphasizing prominent motion patterns and frame-centric actions, they often\\nfall short when dealing with real-world videos. These videos [68, 62, 34, 38, 53, 15, 2, 58], typically\\nrecorded in indoor settings and encompassing Activities of Daily Living (ADL), present challenges\\nthat these transformers are not designed to handle. The challenges of ADL typically includes subtle\\nmotion, videos captured from multiple camera viewpoints, and actions with similar appearance. To\\naddress these challenges of ADL, studies [71, 56, 13, 28] have been conducted on pure-pose based\\napproaches that utilizes 2D and 3D poses. These approaches are effective on datasets recorded in\\nlaboratory settings [53, 38, 69] where human actions are not spontaneous. However, they struggle\\nwith real-world videos [17, 15] that necessitate appearance modeling of the scene to incorporate\\nobject encoding. In response, various approaches [15–17, 4, 5] have integrated RGB and pose\\nmodalities to model ADL. Notably, these methods typically utilize 3D poses, which are dependent on\\ndepth sensors or computationally intensive RGB algorithms [48, 46]. In contrast, our methods, PAAB\\nand PAAT, leverage 2D poses, which are generally accurate and easier to obtain. The closest to our\\nwork [74, 43] perform multi-tasking for pose estimation and action recognition by sharing a 3D CNN\\nencoder with multiple heads. Unlike these, PAAT’s multitasking (patch-keypoint prediction task) is\\ntailored for use in ViTs and intriguingly, it exhibits higher effectiveness at the initial layers rather\\nthan the final layers. To our knowledge, this is the first attempt to learn a pose-aware representation\\nusing vision transformers.\\n6 Conclusion\\nIn conclusion, we proposed PAAB and PAAT, two methods for learning pose-aware representations\\nwith ViTs in the first attempt at combining the RGB and 2D pose modalities into a single-stream ViT.\\nBased on our extensive experimental analysis, we find that incorporating pose information leads to\\ngeneralized ViTs that are effective across multiple tasks, and even across datasets. Surprisingly, we\\nobserve that our methods do not significantly alter the attention distributions of the backbone ViTs,\\nand instead rely on the feed-forward layers of the model to learn pose-aware representations.\\nAs for which method to use, we recommend end users to prefer PAAT over PAAB due to its consistent\\nsuperior performance, enhanced generalizability and its ability to learn more fine-grained pose\\nrepresentations. PAAT also demands less computational resources than PAAB during inference,\\n9\\nnecessitating no poses and additional computational parameters. However, PAAB can be a viable\\nchoice under conditions of poor pose quality and absence of computational constraints.\\nFuture research will investigate the utilization of other entity-specific priors, like segmentation masks,\\nto address a broad range of vision tasks.\\nAcknowledgments\\nWe thank the lab members of ML Lab at UNC Charlotte for valuable discussion. We thank Jinghuan\\nShang and Saarthak Kapse for their helpful feedback. This work is supported by the National Science\\nFoundation (IIS-2245652).\\nReferences\\n[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings\\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197,\\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.\\n385. URL \\nhttps://aclanthology.org/2020.acl-main.385\\n.\\n[2] S. Mohsen Amiri, Mahsa T. Pourazad, Panos Nasiopoulos, and Victor C.M. Leung. Nonintrusive human activity monitoring in a smart home environment. In 2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013), pages\\n606–610, 2013. doi: 10.1109/HealthCom.2013.6720748.\\n[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luciˇ c, and Cordelia ´\\nSchmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV), pages 6836–6846, October 2021.\\n[4] Fabien Baradel, Christian Wolf, and Julien Mille. Human action recognition: Pose-based\\nattention draws focus to hands. In 2017 IEEE International Conference on Computer Vision\\nWorkshops (ICCVW), pages 604–613, Oct 2017. doi: 10.1109/ICCVW.2017.77.\\n[5] Fabien Baradel, Christian Wolf, Julien Mille, and Graham W. Taylor. Glimpse clouds: Human\\nactivity recognition from unstructured feature points. In The IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), June 2018.\\n[6] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for\\nvideo understanding? In Proceedings of the International Conference on Machine Learning\\n(ICML), July 2021.\\n[7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet:\\nA large-scale video benchmark for human activity understanding. In Proceedings of the IEEE\\nConference on Computer Vision and Pattern Recognition, pages 961–970, 2015.\\n[8] Zhe Cao, Gines Hidalgo Martinez, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose:\\nRealtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 2019.\\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\\nSergey Zagoruyko. End-to-end object detection with transformers, 2020.\\n[10] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the\\nkinetics dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 4724–4733. IEEE, 2017.\\n[11] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of the devil\\nin the details: Delving deep into convolutional nets. In The British Machine Vision Conference\\n(BMVC), 2014.\\n[12] Chun-Fu Richard Chen et al. Crossvit: Cross-attention multi-scale vision transformer for image\\nclassification. In Proceedings of the International Conference on Computer Vision (ICCV),\\n2021.\\n10\\n[13] Hyung-Gun Chi, Myoung Hoon Ha, Seunggeun Chi, Sang Wan Lee, Qixing Huang, and Karthik\\nRamani. Infogcn: Representation learning for human skeleton-based action recognition. In\\n2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages\\n20154–20164, 2022. doi: 10.1109/CVPR52688.2022.01955.\\n[14] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,\\nrobotics and machine learning. \\nhttp://pybullet.org\\n, 2016–2019.\\n[15] Srijan Das, Rui Dai, Michal Koperski, Luca Minciullo, Lorenzo Garattoni, Francois Bremond, and Gianpiero Francesca. Toyota smarthome: Real-world activities of daily living. In\\nProceedings of the International Conference on Computer Vision (ICCV), 2019.\\n[16] Srijan Das, Saurav Sharma, Rui Dai, Francois Bremond, and Monique Thonnat. Vpn: Learning\\nvideo-pose embedding for activities of daily living. In European Conference on Computer\\nVision, pages 72–90. Springer, 2020.\\n[17] Srijan Das, Rui Dai, Di Yang, and Francois Bremond. Vpn++: Rethinking video-pose embeddings for understanding activities of daily living. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, pages 1–1, 2021. doi: 10.1109/TPAMI.2021.3127885.\\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\\nJakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image\\nrecognition at scale. Proceedings of the International Conference on Learning Representations\\n(ICLR), 2021.\\n[19] Yong Du, Yun Fu, and Liang Wang. Skeleton based action recognition with convolutional\\nneural network. In 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), pages\\n579–583, 2015. doi: 10.1109/ACPR.2015.7486569.\\n[20] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\\nChristoph Feichtenhofer. Multiscale vision transformers. In ICCV, 2021.\\n[21] Haoshu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person pose\\nestimation. 2017 IEEE International Conference on Computer Vision (ICCV), pages 2353–2362,\\n2016.\\n[22] Christoph Feichtenhofer. X3D: expanding architectures for efficient video recognition. In 2020\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle,\\nWA, USA, June 13-19, 2020, pages 200–210. Computer Vision Foundation / IEEE, 2020. doi:\\n10.1109/CVPR42600.2020.00028. URL \\nhttps://openaccess.thecvf.com/content_\\n\\nCVPR_2020/html/Feichtenhofer_X3D_Expanding_Architectures_for_Efficient_\\nVideo_Recognition_CVPR_2020_paper.html.\\n[23] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network\\nfusion for video action recognition. In Computer Vision and Pattern Recognition (CVPR), 2016\\nIEEE Conference on, pages 1933–1941. IEEE, 2016.\\n[24] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks\\nfor video recognition. CoRR, abs/1812.03982, 2018. URL \\nhttp://arxiv.org/abs/1812\\n.\\n03982.\\n[25] Pallabi Ghosh, Yi Yao, Larry S Davis, and Ajay Divakaran. Stacked spatio-temporal graph\\nconvolutional networks for action segmentation. arXiv preprint arXiv:1811.10575, 2018.\\n[26] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne\\nWestphal, Heuna Kim, Valentin Haenel, Ingo Fründ, Peter Yianilos, Moritz Mueller-Freitag,\\nFlorian Hoppe, Christian Thurau, Ingo Bax, and Roland Memisevic. The \"something something\"\\nvideo database for learning and evaluating visual common sense. CoRR, abs/1706.04261, 2017.\\nURL \\nhttp://arxiv.org/abs/1706.04261\\n.\\n11\\n[27] Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid,\\nand Jitendra Malik. AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual\\nActions. Conference on Computer Vision and Pattern Recognition(CVPR), 2018.\\n[28] Ryo Hachiuma, Fumiaki Sato, and Taiki Sekii. Unified keypoint-based action recognition\\nframework via structured keypoint pooling. arXiv preprint arXiv:2303.15270, 2023.\\n[29] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer\\nin transformer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,\\nAdvances in Neural Information Processing Systems, 2021. URL \\nhttps://openreview.net/\\n\\nforum?id=iFODavhthGZ.\\n[30] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the\\nhistory of 2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision\\nand Pattern Recognition, pages 6546–6555, 2018.\\n[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 770–778, 2016.\\n[32] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human\\naction video dataset. arXiv preprint arXiv:1705.06950, 2017.\\n[33] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Ahmed Sohel, and Farid Boussaïd.\\nA new representation of skeleton sequences for 3d action recognition. In 2017 IEEE Conference\\non Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,\\n2017, pages 4570–4579. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.486. URL\\n\\nhttps://doi.org/10.1109/CVPR.2017.486\\n.\\n[34] Hema Swetha Koppula, Rudhir Gupta, and Ashutosh Saxena. Learning human activities and\\nobject affordances from rgb-d videos. In IJRR, 2013.\\n[35] Hildegard Kuehne, Hueihan Jhuang, Estíbaliz Garrote, Tomaso Poggio, and Thomas Serre.\\nHMDB: a large video database for human motion recognition. In 2011 International Conference\\non Computer Vision, pages 2556–2563. IEEE, 2011.\\n[36] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, and\\nChristoph Feichtenhofer. Mvitv2: Improved multiscale vision transformers for classification\\nand detection. In CVPR, 2022.\\n[37] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, 2019.\\n[38] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang, Ling-Yu Duan, and Alex C. Kot. Ntu\\nrgb+d 120: A large-scale benchmark for 3d human activity understanding. IEEE Transactions\\non Pattern Analysis and Machine Intelligence, 2019. doi: 10.1109/TPAMI.2019.2916873.\\n[39] Mengyuan Liu, Hong Liu, and Chen Chen. Enhanced skeleton visualization for view invariant\\nhuman action recognition. Pattern Recognition, 68:346 – 362, 2017. ISSN 0031-3203. doi: https:\\n\\n//doi.org/10.1016/j.patcog.2017.02.030\\n. URL \\nhttp://www.sciencedirect.com/science/\\n\\narticle/pii/S0031320317300936.\\n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\\nof the International Conference on Computer Vision (ICCV), 2021.\\n[41] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin\\ntransformer. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\\npages 3192–3201, 2021.\\n12\\n[42] Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, and Wanli Ouyang. Disentangling\\nand unifying graph convolutions for skeleton-based action recognition. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 143–152, 2020.\\n[43] Diogo C Luvizon, David Picard, and Hedi Tabia. 2d/3d pose estimation and action recognition\\nusing multitask deep learning. In Proceedings of the IEEE conference on computer vision and\\npattern recognition, pages 5137–5146, 2018.\\n[44] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni,\\nLi Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning\\nfrom offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298,\\n2021.\\n[45] Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra, Florian Metze, Christoph\\nFeichtenhofer, Andrea Vedaldi, and João F. Henriques. Keeping your eye on the ball:\\nTrajectory attention in video transformers. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural\\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual,\\npages 12493–12506, 2021. URL \\nhttps://proceedings.neurips.cc/paper/2021/hash/\\n\\n67f7fb873eaf29526a11a9b7ac33bfac-Abstract.html.\\n[46] Dario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose\\nestimation in video with temporal convolutions and semi-supervised training. In Conference on\\nComputer Vision and Pattern Recognition (CVPR), 2019.\\n[47] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks, 2016.\\n[48] Grégory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. LCR-Net++: Multi-person 2D\\nand 3D Pose Detection in Natural Images. IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, 2019.\\n[49] Michael S. Ryoo, AJ Piergiovanni, Juhana Kangaspunta, and Anelia Angelova. Assemblenet++:\\nAssembling modality representations via attention connections. In Proceedings of the European\\nConference on Computer Vision (ECCV), 2020.\\n[50] Javier Selva, Anders S. Johansen, Sergio Escalera, Kamal Nasrollahi, Thomas Baltzer Moeslund,\\nand Albert Clap’es. Video transformers: A survey. IEEE transactions on pattern analysis and\\nmachine intelligence, PP, 2022.\\n[51] Pierre Sermanet, Corey Lynch, Jasmine Hsu, and Sergey Levine. Time-contrastive networks:\\nSelf-supervised learning from multi-view observation. In 2017 IEEE Conference on Computer\\nVision and Pattern Recognition Workshops (CVPRW), pages 486–487, 2017. doi: 10.1109/\\nCVPRW.2017.69.\\n[52] Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey\\nLevine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In\\nIEEE International Conference on Robotics and Automation (ICRA), pages 1134–1141. IEEE,\\n2018.\\n[53] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang. Ntu rgb+d: A large scale dataset\\nfor 3d human activity analysis. In Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR), June 2016.\\n[54] Jinghuan Shang and Michael S. Ryoo. Self-supervised disentangled representation learning for\\nthird-person imitation learning. In IEEE/RSJ International Conference on Intelligent Robots\\nand Systems (IROS), pages 214–221, 2021. doi: 10.1109/IROS51168.2021.9636363.\\n[55] Jinghuan Shang, Srijan Das, and Michael S Ryoo. Learning viewpoint-agnostic visual representations by recovering tokens in 3d space. In Advances in Neural Information Processing\\nSystems, 2022.\\n13\\n[56] Lei Shi, Yifan Zhang, Jian Cheng, and Hanqing Lu. Skeleton-based action recognition with\\nmulti-stream adaptive graph convolutional networks. IEEE Transactions on Image Processing,\\n29:9532–9545, 2020.\\n[57] Jamie Shotton, Andrew Fitzgibbon, Mat Cook, Toby Sharp, Mark Finocchio, Richard Moore,\\nAlex Kipman, and Andrew Blake. Real-time human pose recognition in parts from single depth\\nimages. In CVPR 2011, pages 1297–1304, 2011. doi: 10.1109/CVPR.2011.5995316.\\n[58] Gunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav\\nGupta. Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding. In\\nEuropean Conference on Computer Vision(ECCV), 2016.\\n[59] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action\\nrecognition in videos. In Advances in neural information processing systems, pages 568–576,\\n2014.\\n[60] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101\\nhuman actions classes from videos in the wild. CoRR, abs/1212.0402, 2012. URL http:\\n\\n//arxiv.org/abs/1212.0402\\n.\\n[61] Robin Strudel, Ricardo Garcia Pinel, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. 2021 IEEE/CVF International Conference on Computer\\nVision (ICCV), pages 7242–7252, 2021.\\n[62] Jaeyongand Sung, Colin Ponce, Bart Selman, and Ashutosh Saxena. Human activity detection\\nfrom rgbd images. In AAAI workshop, 2011.\\n[63] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer\\nVision and Pattern Recognition (CVPR), pages 2818–2826, 2016. doi: 10.1109/CVPR.2016.308.\\n[64] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\\nHerve Jegou. Training data-efficient image transformers: Distillation through attention. In\\nProceedings of the International Conference on Machine Learning (ICML), volume 139, pages\\n10347–10357, July 2021.\\n[65] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning\\nspatiotemporal features with 3d convolutional networks. In Proceedings of the 2015 IEEE\\nInternational Conference on Computer Vision (ICCV), ICCV ’15, pages 4489–4497, Washington,\\nDC, USA, 2015. IEEE Computer Society. ISBN 978-1-4673-8391-2. doi: 10.1109/ICCV.2015.\\n510. URL \\nhttp://dx.doi.org/10.1109/ICCV.2015.510\\n.\\n[66] Gül Varol, Ivan Laptev, Cordelia Schmid, and Andrew Zisserman. Synthetic humans for action\\nrecognition from unseen viewpoints. International Journal of Computer Vision, 129:2264 –\\n2287, 2019.\\n[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\\nprocessing systems, pages 5998–6008, 2017.\\n[68] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan. Mining Actionlet Ensemble for Action\\nRecognition with Depth Cameras. In IEEE International Conference on Computer Vision and\\nPattern Recognition (CVPR), 2012.\\n[69] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-Chun Zhu. Cross-view action modeling, learning, and recognition. In 2014 IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 2649–2656, June 2014. doi: 10.1109/CVPR.2014.339.\\n[70] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin P. Murphy. Rethinking\\nspatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In European\\nConference on Computer Vision, 2017.\\n14\\n[71] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Thirty-second AAAI conference on artificial intelligence,\\n2018.\\n[72] Bruce Yu, Yan Liu, Xiang Zhang, Sheng-hua Zhong, and Keith Chan. Mmnet: A model-based\\nmultimodal network for human action recognition in rgb-d videos. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, PP:1–1, 05 2022. doi: 10.1109/TPAMI.2022.3177813.\\n[73] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay,\\nJiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch\\non imagenet. In Proceedings of the IEEE/CVF international conference on computer vision,\\npages 558–567, 2021.\\n[74] Mohammadreza Zolfaghari, Gabriel L Oliveira, Nima Sedaghat, and Thomas Brox. Chained\\nmulti-stream networks exploiting pose, motion, and appearance for action classification and\\ndetection. In Computer Vision (ICCV), 2017 IEEE International Conference on, pages 2923–\\n2932. IEEE, 2017.\\n15\\nAppendix\\nA Datasets and Protocol description\\nAction recognition For the task of action recognition we evaluate our methods on three popular\\nActivities of Daily Living (ADL) datasets. Toyota-Smarthome[15] (Smarthome, SH) provides 16.1k\\nvideo clips of elderly individuals performing actions in real-world settings. The dataset contains\\n18 subjects, 7 camera views, and 31 action classes. For evaluation, we follow the cross-subject\\n(CS) and cross-view (CV1, CV2) protocols. Due to the unbalanced nature of the dataset, we use\\nthe mean class-accuracy (mCA) performance metric. The dataset provides 2D skeletons containing\\n13 keypoints that were extracted using LCRNet [48], which we use to generate the pose maps for\\nour method. NTU-RGB+D[53] (NTU) provides 56.8k video clips of subjects performing actions\\nin a laboratory setting. The dataset consists of 40 subjects, 3 camera views, and 60 action classes.\\nFor evaluation, we follow the cross-view-subject (CVS) protocols proposed in [66] and evaluate\\nperformance with top-1 classification accuracy. In the CVS protocols, only the 0\\n◦ view from the\\nCS training split is used for training, while testing is carried out on the 0\\n◦\\n, 45◦\\n, and 90◦ view from\\nthe CS test split, which are referred to as CVS1, CVS2, and CVS3. We use the CVS protocols\\nbecause they better represent the cross-view challenge. The dataset provides 2D skeletons containing\\n25 keypoints extracted using Randomized Decision Forest [57], which we use to generate the pose\\nmaps. Northwestern-UCLA Multiview activity 3D Dataset[69] (N-UCLA) contains 1200 video\\nclips of subjects performing actions in a laboratory setting. The dataset consists of 10 subjects, 3\\ncamera views, and 10 action classes. For evaluation, we follow the cross-view (CV) protocols in\\nwhich the model is trained on two camera views and tested on the remaining view. For example, the\\nCV3 protocol indicates the model was trained on views 1, 2 and tested on view 3. For evaluation,\\nwe report the accuracy on CV3 and the average accuracy on all cross-view protocols. We employ\\nOpenPose [8] to extract 2D skeletons containing 18 keypoints and to generate the pose maps.\\nChoice of Action Recognition Datasets and protocols Contrary to popular action recognition\\nmethods evaluated on datasets like Kinetics [32] and SSV2 [26], our method targets scenarios emphasizing human poses, which we argue are crucial in Activities of Daily Living. This consideration\\ninfluences our choice of datasets. Given that datasets like Kinetics often position humans centrally,\\nclose to the camera source, many poses remain obscured, thereby minimizing the relevance of skeletal\\ndata in these contexts [71].\\nFor evaluation on NTU dataset, we follow the CVS protocols since they are challenging owing to the\\ndisparity in the training distribution. While most methodologies evaluate this dataset using Crosssubject (CS) and Cross-view (CV) protocols, these tend to be less rigorous, saturated, and do not\\nreflect real-world scenarios. The performance of PAAB and PAAT enhance the baseline TimeSformer\\nmetrics on CS and CV protocols by a slight margin of 0.3%-0.5%, indicating that pose-aware RGB\\nrepresentations do not necessarily provide an additional performance boost. Nevertheless, the efficacy\\nof the pose-aware representation, as learned from the pre-trained NTU (CS), is manifested in its\\ngeneralizability for video retrieval tasks (see Fig. 5 in the main paper).\\nB Dataset specific Implementation details\\nWe train all of our video models (TimeSformer based) on 8 RTX A5000 GPUs with a batch size of\\n32 for Smarthome and 64 for NTU and NUCLA. We train the image models (DeiT based) used in\\nmulti-view robotic video alignment on a single RTX A5000 GPU, with a batch size of 1.\\nAction recognition In all experiments, we follow a training pipeline similar to [6]. The RGB inputs\\nto our models are video frames with a size of 8 × 224 × 224 for Smarthome and NUCLA and a\\nsize of 16 × 224 × 224 for NTU. Frames are sampled at a rate of 1\\n32 for Smarthome and 1\\n4\\nfor NTU\\nand NUCLA. To ensure that the video frames input to our model will contain pose keypoints, prior\\nto sampling frames we first extract a 224 × 224 crop from the video that contains only the human\\nsubject. This can be done using the pose keypoints extracted from the RGB or by using a pre-trained\\nhuman detector [47]. Our backbone model in which we insert PAAB and PAAT is a Kinetics-400\\n[32] pre-trained TimeSformer [6] model. For fine-tuning, we train the models for 15 epochs.\\nMulti-view robotic video alignment We use DeiT as a backbone architecture for inserting PAAT\\nand PAAB and use the time-contrastive loss [51] to train our models. The training enforces that\\n16\\nthe distance between video frames are close if the frames are temporally close, but far if they are\\ntemporally distant. We train all of our models from scratch and follow the training recipe provided\\nin [55].\\nC Model Diagnosis\\nFigure 8: Effects of pre-training PAAB and PAAT\\nwith and without Kinetics 400 (K400) pre-training.\\nMethod Pretraining mCA\\nPAAB None 71.42\\nPAAB K400 69.98\\nPAAT None 72.54\\nPAAT K400 72.25\\n1 2 3 4 5\\nLoss Scale\\n70\\n75\\n80\\n85\\nAccuracy (%)\\nSH (CS)\\nNTU (CVS1)\\nFigure 9: Ablation on the loss scale (λ) of\\nPAAT\\nVarying loss scale In Figure 9 we present the results of varying PAAT’s loss scaling factor, λ. We\\ntrain PAAT on the Smarthome cross-subject (SH CS) and NTU CVS1 protocols with the following\\nvalues of λ: 0.3, 0.6, 1.0, 1.3, 1.6, 2.0, 5.0. We find that on both datasets, a value of λ = 1.6 is\\noptimal for training PAAT.\\nPre-training with Kinetics Surprisingly, our methods do not require Kinetics pre-training to\\nachieve good performance. In Figure 8, we present the results of pre-training PAAB and PAAT on\\nKinetics-400 [32] prior to finetuning them on the Smarthome cross-subject protocol. Even more\\ninteresting is our observation that Kinetics pre-training degrades the performance of PAAB and PAAT.\\nDuring Kinetics pre-training, we train the backbone without the incorporation of input poses. For\\nPAAB, the additional block performs attention across all patches. The observed degradation in action\\nclassification performance may be attributed to discrepancies between pre-training and fine-tuning\\nstages. This warrants the necessity of collecting more pose based real-world action recognition\\ndatasets.\\nFFN feature distances In Figure 10, we show that PAAB and PAAT rely on the feed-forward\\nnetworks (FFNs) within the model to learn pose-aware representations. We report the average feature\\ndistance between tokens before and after the FFNs at different layers of the backbone transformer. We\\nfind that in the initial layers, both PAAB and PAAT follow a similar trend to the baseline. However\\naround layer 6, we observe that the FFNs starts influencing the token representation. We recall\\nthat the attention distribution in PAAB and PAAT resembles with the baseline transformer. In this\\nexperiment, we find that both PAAB and PAAT alter the token representation more than the baseline,\\nindicating that they are leveraging the FFNs rather than the attention distribution to learn pose-aware\\nrepresentations. This analysis reveals that the intermediate layers in transformers, particularly the\\n2 4 6 8 10 12\\nTransformer layer\\n0\\n25\\n50\\n75\\n100\\nFeature Distance\\nTimeSformer (baseline)\\nPAAB (ours)\\nPAAT (ours)\\nFigure 10: Average feature distance between tokens before and after the feed-forward networks.\\n17\\nFeed-Forward Networks (FFNs), play a pivotal role in learning pose-aware representation. However,\\nthis does not necessarily imply that PAAB and PAAT need to be integrated within these middle layers.\\nD Fusion of PAAB and PAAT\\nWhile we have discussed PAAB and PAAT as two different strategies for learning pose-aware video\\nrepresentation, we also explore their combinination within a single architecture. In this experiment,\\nwe plug-in a PAAB following layer 12 of the backbone TimeSformer and invoke a keypoint-classifier\\nbased PAAT block after layer 1. Using a loss scaling factor of λ = 1.6, we observe a Smarthome\\n(CS) action classification accuracy of 69.9%, as compared to 71.4% and 72.5% achieved by PAAB\\nand PAAT respectively. We argue that this drop in performance is owing to the conflicting gradients\\ngenerated by the introduction of both modules (PAAB & PAAT).\\nE Limitations\\nAs previously mentioned, PAAB and PAAT are two distinct yet complementary methods for learning\\npose-aware representation, each possessing their unique strengths and weaknesses. The exploration\\nof a strategy to integrate the benefits of both methods remains an open challenge. The amalgamation\\nof both methods into a single model could potentially yield a more robust framework compared to\\nindividual models.\\nAdditionally, another key limitation of PAAB and PAAT is their dependency on pose data during\\ntraining. Although recent advances have facilitated pose extraction from RGB [8], the associated\\ncomputational costs remain a challenge for training our methodologies. Moreover, PAAB necessitates pose information during inference, along with the inclusion of additional parameters, thereby\\nextending the inference time.\\n18', doc_id='dc1a1edd-f90d-4463-b1da-2e18e6d949c3', embedding=None, doc_hash='cad63334042c9f061710fb339112f0bdbd13724e7639fcd47eef5204b24af614', extra_info={'catgeory': 'CV', 'filename': 'CV'}),\n",
       " Document(text='SPECIAL SECTION ON INNOVATION AND APPLICATION OF INTELLIGENT PROCESSING OF\\nDATA, INFORMATION AND KNOWLEDGE AS RESOURCES IN EDGE COMPUTING\\nReceived February 4, 2020, accepted February 12, 2020, date of publication February 14, 2020, date of current version March 17, 2020.\\nDigital Object Identifier 10.1109/ACCESS.2020.2974101\\nFeature Extraction and Analysis of Natural\\nLanguage Processing for Deep Learning English\\nLanguage\\nDONGYANG WANG 1\\n, JUNLI SU 2\\n, AND HONGBIN YU 3\\n1College of Education, Arts and Science, Lyceum of the Philippines University, Batangas City 4200, Philippines\\n2Department of Elementary Education, Jiaozuo Teachers College, Jiaozuo 454002, China\\n3School of Digital Media, Jiangnan University, Wuxi 214122, China\\nCorresponding author: Dongyang Wang (\\n18691916989@163.com\\n)\\nABSTRACT NLP (Natural Language Processing) is a technology that enables computers to understand\\nhuman languages. Deep-level grammatical and semantic analysis usually uses words as the basic unit,\\nand word segmentation is usually the primary task of NLP. In order to solve the practical problem of\\nhuge structural differences between different data modalities in a multi-modal environment and traditional\\nmachine learning methods cannot be directly applied, this paper introduces the feature extraction method of\\ndeep learning and applies the ideas of deep learning to multi-modal feature extraction. This paper proposes\\na multi-modal neural network. For each mode, there is a multilayer sub-neural network with an independent\\nstructure corresponding to it. It is used to convert the features in different modes to the same-modal\\nfeatures. In terms of word segmentation processing, in view of the problems that existing word segmentation\\nmethods can hardly guarantee long-term dependency of text semantics and long training prediction time,\\na hybrid network English word segmentation processing method is proposed. This method applies BI-GRU\\n(Bidirectional Gated Recurrent Unit) to English word segmentation, and uses the CRF (Conditional Random\\nField) model to annotate sentences in sequence, effectively solving the long-distance dependency of text\\nsemantics, shortening network training and predicted time. Experiments show that the processing effect of\\nthis method on word segmentation is similar to that of BI-LSTM-CRF (Bidirectional- Long Short Term\\nMemory-Conditional Random Field) model, but the average predicted processing speed is 1.94 times that\\nof BI-LSTM-CRF, effectively improving the efficiency of word segmentation processing.\\nINDEX TERMS Feature extraction, English word segmentation processing, long short term memory, gated\\nrecurrent unit.\\nI. INTRODUCTION\\nWith the rapid development of Internet information technology and the continuous advancement of science and technology, a large amount of data of various types and structures\\nhave been accumulated in the real life and scientific research\\nfields. In the real world, for the observation target of the same\\nsemantic conceptual ontology, multiple observation methods\\ncan often be used to obtain data information from multiple\\ndifferent observation channels, and these data from different\\ninformation channels describe the same concept. Each of\\nthese kinds of information data can be called a different\\nmodal, or different observation perspectives. Different inforThe associate editor coordinating the review of this manuscript and\\napproving it for publication was Ying Li.\\nmation modalities together constitute multi-modal data for\\nthe same problem. NLP (Natural Language Processing) is\\none of the key technologies for realizing human-computer\\ninteraction and artificial intelligence [1]–[3]. It is listed as the\\nthree major elements of artificial intelligence research with\\nvoice processing and image processing [4], [5]. In the early\\ndays of NLP research, the main focus was on the analysis\\nof language structure, technology-driven machine translation,\\nand language recognition [6]–[9]. The current focus is on\\nhow NLP can be used in the real world. The corresponding\\nresearch areas include dialogue systems and social media\\ndata. However, the training of deep frames is a difficult task,\\nand traditional shallow proven methods that have proven\\neffective cannot be transplanted into deep learning to ensure\\ntheir effectiveness [10]–[13]. Another realistic problem is that\\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see \\nhttps://creativecommons.org/licenses/by/4.0/\\n 46335\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nthere is no necessary connection between increasing the layer\\nstructure and obtaining better feature representations. For\\nexample, in a neural network, the more hidden layers, the less\\nimpact the first layer in the backpropagation algorithm. When\\nusing the gradient descent algorithm, it will also fall into the\\nlocal optimum and lose the effect of continued transmission.\\nRelated scholars have proposed a word segmentation algorithm based on supervised machine learning. This method\\nimplements a word-based word segmentation system. The\\nmain innovation is to use the maximum entropy model as a\\ntokenizer to automatically label characters. This method has\\nthe highest recall rate of 72.9% in the AS2003 closed test\\nexperiment [14]–[17]. In the method of English word segmentation based on the dictionary and rules, it mainly focuses\\non the word segmentation algorithm and dictionary structure [18], [19]. The advantages of dictionary-based and rulebased methods are simple, easy to implement, and suitable\\ndictionaries can be formulated according to special scenarios.\\nIn addition, in systems that require real-time performance,\\ndictionary-based and rule-based methods are often more suitable because of their high efficiency [20], [21]. The disadvantages are: there is a problem of word segmentation ambiguity;\\nthere is no universal standard for word division, so the quality\\nof the dictionary cannot be clearly defined. The dictionary has\\na great impact on the segmentation result [22]–[25]. With the\\nadvent of the era of big data, data has become more and more\\nin natural language processing problems [26], [27]. Improving these labeling problems to support parallel computing\\nand being able to perform parallel learning on large-scale\\ntraining data has also become a research hotspot [28]–[32].\\nParallel learning is currently supported, including maximum\\nentropy models and conditional random field models. Some\\nresearchers have proposed the technical route of ‘‘understand\\nfirst and then segmentation’’ [33]–[35]. The idea of understanding the segmentation first is to solve the lack of global\\ninformation in the traditional matching segmentation, while\\nthe statistical method lacks the structural information of the\\nsentence [36]–[39]. Relevant scholars use deep learning to\\nperform sequence labeling in the NLP field [40], [41]. It can\\nalso add a sequence labeling model to combine with the\\noutput of the previous neural network to extract the best labeling sequence through the Viterbi algorithm. Related scholars\\nhave proposed an open domain question answering system\\nbased on relationship matching [42], [43]. The problem analysis problem based on relation matching is solved through\\nthe associated data in the question answering system. The\\nfragments in the question match the binary relationship in the\\ntriples and are automatically collected using the relational text\\npattern. Existing models do not take into account the importance of different modalities for the current learning task, but\\nonly focus on how to effectively use multiple modalities for\\nfeature extraction at the same time. Moreover, the selection of\\nmodals and the filtering of harmful modals are not involved,\\nand this issue is also an important issue addressed in this\\npaper. In terms of word segmentation processing, in view of\\nthe problems that existing word segmentation methods can\\nhardly guarantee long-term dependency of text semantics and\\nlong training prediction time, a hybrid network English word\\nsegmentation processing method is proposed. Experimental\\nresults show that this method improves the efficiency of\\nnatural language processing.\\nIn terms of English word segmentation, since traditional\\nmachine learning methods cannot solve the long-distance\\ndependencies of texts, it is difficult to analyze the information\\ncontained in the problem as a whole and grasp the user’s\\ntrue intention. In order to solve the above problems and save\\nthe relevance of the forward and reverse information of the\\ntext, this paper uses BI-GRU (Bidirectional Gated Recurrent\\nUnit) neural network and combines the CRF (Conditional\\nRandom Field) model to solve the problem of sequence labeling at the sentence level analysis, based on BI-GRU-CRF\\n(Bidirectional-Gated Recurrent Unit- Conditional Random\\nField) hybrid network English word segmentation processing method. Specifically, the technical contributions of this\\narticle are summarized as follows:\\nFirstly, a multimodal fusion feature extraction method is\\nproposed. The problem of heterogeneity of multi-modal data\\nis solved through the feature transformation of deep neural\\nnetworks.\\nSecondly, in view of the problems that traditional neural\\nnetwork models cannot capture the long-distance dependencies of text and the long cost of training and prediction of\\nLSTM (Long Short Term Memory) neural networks, a word\\nsegmentation processing method based on BI-GRU-CRF\\nhybrid network is proposed.\\nThirdly, the proposed method is tested from two aspects,\\naccuracy and timeliness. According to these two sets of\\nexperiments, the proposed hybrid network word segmentation processing method has good performance in English\\nword segmentation processing.\\nThe rest of this article is organized as follows:\\nSection 2 analyzes English language feature extraction\\nbased on semi-supervised multi-modal neural networks.\\nSection 3 studies the processing of English word segmentation based on BI-GRU-CRF hybrid network. Finally, the full\\ntext is summarized in Section 4.\\nII. ENGLISH LANGUAGE FEATURE EXTRACTION BASED\\nON SEMI-SUPERVISED MULTI-MODAL NEURAL\\nNETWORK\\nA. NATURAL LANGUAGE PROCESSING\\nTEXT SEGMENTATION\\nWhen a computer processes text information, it is mainly a\\nprocessing operation of words in the text information. The\\nfirst problem it faces is the segmentation of words. For each\\nNLP process, word segmentation plays a vital role. Word segmentation can separate written or spoken text into meaningful\\nword tags, and determine the division range between words\\nin English spaces, so the division between words is obvious.\\nConversely, there is no space between words in the English\\ntext. At this time, the computer must separate the words in\\n46336 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nthe text to be able to correctly process the text content. It\\ncan be seen that text segmentation is an important part of\\nNLP, especially when the amount of text information to be\\nprocessed is large, the scope of the content will be wider.\\nAt this time, there will be many professional vocabulary,\\nhow to correctly identify segmenting text information is an\\nimportant factor that affects the performance of the word\\nsegmentation system.\\nWhen processing English text information, words are the\\nmost basic unit. Generally speaking, the research content\\nin terms of semantics and grammar, etc., must use words\\nas the smallest unit. Words are the central problem of text\\ninformation mining. With the help of a computer, the words\\nin the text content are segmented one by one to analyze the\\ntext content. Therefore, automatic word segmentation of text\\nis the premise of research and analysis such as automatic\\ninformation retrieval, automatic information extraction, and\\nnatural language understanding.\\nSince there are spaces between English words, the basic\\nmethod of word segmentation in English text is introduced\\nhere:\\n(1) Word-by-word traversal matching method: It matches\\nthe text content with the words stored in the dictionary one\\nby one, until all the words in the text content are segmented\\nout. Because this method needs to compare all the words in\\nthe dictionary one by one, it takes time, the computer is slow,\\nand the word segmentation efficiency is not high.\\n(2) Maximum matching method: The vocabulary table is\\nstored in the computer, which is called the bottom table.\\nA text string of a specific length is selected according to a\\npredetermined sequence of text content, and the length of the\\nstring is called a maximum word length. Compared the words\\nin the bottom table with the maximum word length, if the\\ncomparison result is the same, you can say that the string is\\na word, and the pointer of the computer continues to move\\nbackwards. The number of words with the maximum word\\nlength continues to follow.\\n(3) Reverse maximum matching method: It is basically\\nthe same as the principle of the maximum matching method,\\nthe main difference is that the scanning direction is opposite\\nwhen comparing, that is: if the scanning is from left to right\\nwhen the maximum matching method is compared, the maximum matching method is reversed.\\n(4) Two-way scanning method: This method is a word\\nsegmentation method combining the above (2) and (3). Compared the segmentation results of the two, if the results of the\\ntwo methods are the same, it means that the results are correct,\\notherwise it is regarded as a doubt. This requires choosing\\na more accurate word segmentation method based on text\\ncontent or manual intervention. Because this method is a twoway scan, it will consume more time and cost.\\nPART-OF-SPEECH TAGGING\\nAfter the text is tokenized, you can then analyze the part of\\nspeech of each word. Part-of-speech tagging refers to the process of assigning a part-of-speech or tagging the vocabulary\\ntype in text to each word in text information. It uses a computer to identify the part-of-speech of each word in the text.\\nBecause there are many vocabularies in English texts, and\\nthe same words may have different parts of speech in different\\nlanguage paragraphs, and the combinations between words\\nare various, the results of using the rule method in English\\ntexts may not be ideal. Commonly used English Part-ofspeech tagging is a statistical method.\\nThe computer counts all the text information to get the\\nprobability of label co-occurrence and the probability that the\\nword represents a certain part of speech.\\nFEATURE EXTRACTION\\nData is represented by a fixed number of characteristics,\\nwhich can be binary, categorical, or continuous. Characteristics are synonyms for input variables or attributes. Finding a\\ngood data representation as an effective feature measurement\\nis crucial.\\nThere are various ‘‘sizes’’ and ‘‘forms’’ of data in text\\ninformation. An important point when extracting features\\nin text is structured data. Generally, the raw data that has\\nnot been processed in the text is converted into structured\\ndata. The process of tools to get valid information is called\\ninformation extraction.\\nFeature extraction solves the problem of finding the most\\ncompact and informative feature set. For classification and\\nregression problems, defining feature vectors remains the\\nmost common and convenient method of data representation.\\nIt stores data in simple tables, and each feature is the result\\nof a quantitative or qualitative measurement, which is an\\n‘‘attribute’’ or ‘‘variable’’. Feature selection is a key technology for processing high-dimensional data and has applications in industrial detection and diagnostic systems, speech\\nrecognition, biotechnology, the Internet, targeted marketing,\\nand many other emerging applications. The commonly used\\nfeature extraction method is driven by the size of the data\\ntable. With the increasing efficiency of data storage, the size\\nof data tables is also increasing. Extracting effective features\\nfrom text and avoiding useless data processing is the key to\\nexperimental research.\\nB. NON-LINEAR MAPPING METHOD IN SINGLE-MODE\\nENVIRONMENT FOR LOW-DIMENSIONAL FEATURE\\nEXTRACTION\\nFor data sets in which some internal structures are hidden in a\\nhigh-dimensional nonlinear space, the effect of such methods\\nis not obvious. As a new non-linear feature transformation\\nmethod, the flow pattern learning method learns the flow\\npattern structure of the original data from the original highdimensional feature space, and maps it to the new feature\\nspace of lower dimensions. For data sets with complex internal structures, the method of flow pattern embedding learning\\nis often more effective. Common flow pattern learning methods include iso-metric mapping and LLE (Locally Linear\\nEmbedding).\\nVOLUME 8, 2020 46337\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 1. Schematic diagram of the overall structure of a semi-supervised multimodal neural network.\\nThe isometric mapping method assumes that the lowdimensional embedded flow pattern of the data distribution is\\nequidistant from a subset of the Euclidean space, and requires\\nthat the subset be convex. The basis of the isometric mapping algorithm is a multi-dimensional scaling method. When\\nmeasuring the distance between two points, the multidimensional scaling method uses the Euclidean distance, while\\nthe isometric mapping method uses the geodesic distance\\nbetween the two. The isometric mapping algorithm first needs\\nto determine which points on the flow structure are adjacent.\\nFor each point xi and xj\\nin the original space, the distance\\nbetween them is δi,j\\n. For each sample point xi\\n, if the distance\\nto other sample points is less than the threshold ε, or the first\\nk nearest neighbors of the sample xi are connected, these\\nadjacent points are connected to obtain a directed graph G.\\nAssume that the mapping point of sample xi\\nin the new lowdimensional feature space is zi\\n, and the optimization goal of\\nisometric mapping is minP\\ni<j\\n(d(zi,zj) − δi,j)\\n2\\n.\\nThe isometric mapping method uses geodesic distance\\nto measure the distance between two points, and uses the\\nEuclidean distance between two points that are closer when\\ncalculating the distance, and uses the shortest path length\\nbetween the two points between two samples that are farther away. Isometric mapping method is suitable for lowdimensional flow structure with flat data distribution, and is\\nnot suitable for learning flow structure with large curvature\\nand noise. When solving the geodesic distance, the shortest\\ndistance between the sample point pairs needs to be calculated. The calculation complexity is very high. When the\\ndata dimension is high and the number of samples is large,\\nthe calculation time and space is huge.\\nLLE is another common nonlinear dimensionality reduction method. The low-dimensional data obtained by the\\nLLE method can maintain the topological relationship of\\nthe original data space. The LLE method assumes that the\\nlow-dimensional flow pattern in which the data resides is\\nlocally linear, and that each sampling point can be reconstructed by its nearest neighbors. Similar to the isometric\\nmapping method, the LLE method first needs to find the\\nk nearest neighbor sample points of each sample, and then\\nobtain the local reconstruction weights of the sample point\\npairs, and use these sample points to linearly reconstruct\\nthe original high-dimensional data points. Compared with\\nequal-metric mapping, the local linear embedding method\\ncan learn a locally linear low-dimensional manifold structure of any dimension, and it does not need to calculate the\\ndistance of all sample point pairs, which greatly reduces the\\ncomputational cost. However, the LLE method is sensitive\\nto the size selection of the neighbors and is not robust to\\nnoise data.\\nC. MODEL STRUCTURE DESIGN OF MULTIMODAL\\nFUSION FEATURE EXTRACTION BASED ON DEEP\\nLEARNING\\nAs shown in the Figure 1, the overall framework of the model\\nwhich we proposed has a tree structure. It has been divided\\ninto two main parts, the one part is the root network, another\\nis the upper layer network. The BP (Back Propagation) algorithm is used in the parameter training of the root network.\\nThe function loss defined in the auxiliary bridge layer is\\nshared by each sub-network and is used to adjust each subnetwork simultaneously.\\n46338 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 2. Schematic diagram of the root network structure.\\nD. ISOMORPHIC FEATURE EXTRACTION OF THE ROOT\\nNEURAL NETWORK\\nOne of the main problems in multimodal feature learning\\ntasks is the heterogeneity in different data modalities. The\\nraw data of different modalities are in their own different\\nfeature spaces. The key to most multimodal learning methods\\nis to project different modalities into the same subspace.\\nDeep learning models show outstanding advantages in revealing hidden hierarchical feature expressions and performing\\nfeature transformations on raw data. Therefore, the model\\nproposed in this paper makes full use of the characteristics of deep neural networks to eliminate the different data\\nmodalities.\\nFigure 2 shows the basic structure of the root network. Each\\ndata mode corresponds to a sub-network. Here, the number of\\nhidden layers contained in the sub-networks m corresponding\\nto the m-th mode is defined as nm, the i-th hidden layer is\\ndenoted as h\\ni\\nm, and the connection weight between the i-th\\nhidden layer and its lower layer is expressed as w\\ni\\nm.\\nE. SEMI-SUPERVISED FUSION FEATURE LEARNING BASED\\nON AUTOENCODER\\nIn order to effectively utilize the valuable supervision information in the data, the auxiliary optimization layer is actually\\na Logistic classifier. When the top three layers of the network\\nhave been trained, the final loss function is defined as:\\nL = Ldis + λ1 kW kλ2 + 1k Wk\\n3\\nF + βLgen (1)\\nLdis = −X\\nN\\ni\\nlog(P(Y = y\\n(i)\\n)\\nh\\n(i)\\n, btop, V) (2)\\nLgen = −X\\nN\\ni\\nx\\n0\\n(i)\\nlog xˆ\\n0\\n(i) + log(1 − ˆx\\n0\\n(i)\\n) (3)\\nThe entire loss function is divided into three parts: discriminative loss Ldis with supervision, generation loss Lgen with\\nno supervision, and regular terms. The generation loss is used\\nto measure the reconstruction loss between the input x\\n0\\nand\\nthe output xˆ\\n0\\n. The smaller reconstruction error means that the\\nextracted fused features retain more original information. The\\nTABLE 1. Classification accuracy of different multi-modal feature\\nextraction algorithms on experimental data sets.\\nreconstruction loss xˆ\\n0\\nis defined as:\\nxˆ\\n0 = s(WT\\ns(Wx0 + b) + b\\n0\\n) (4)\\ns (·) represents the sigmoid function, and b and b 0\\nare bias\\nterms.\\nThe weight matrix W is learned from both supervised\\nclass information and unsupervised reconstruction input. The\\nparameter β is used to balance the discriminating losses Ldis\\nand Lgen. Two regular terms are introduced to sparse the\\nweight parameter W. When the gradient descent method is\\nused to optimize the training of the model, the regular term\\nkWk\\n1 is not differentiable at zero. In this case, the method of gradient descent is not suitable. For this purpose, the term is approximately smoothed to: kWk\\n =\\nX\\nij\\np\\nWij + σ (5)\\nσ is a small positive real number.\\nF. SIMULATION RESULTS ANALYSIS\\nIn order to evaluate the discrimination ability of multi-modal\\nfusion features extracted using the proposed model, this paper\\nfirst gives the classification accuracy obtained when different\\nalgorithms are applied to the experimental dataset, as shown\\nin Table 1.\\nExperimental results show that the proposed algorithm\\nmodel has the best classification accuracy compared to other\\nalgorithms on the image classification data set used. For the\\nNUS-WIDE-Object dataset, it is not difficult to find that\\nfeatures using text modalities are more accurate than features\\nusing image modality. This also reflects the structural dissimilarity between the features obtained in different modes,\\nand simple splicing may even destroy the original internal\\nstructure. Due to the loss of information of the PCA (Principal\\nComponents Analysis) method, the classification accuracy\\nobtained using the PCA + SVM method is even lower, which\\nalso confirms that the traditional feature extraction method\\ncannot be directly applied to multi-modal data, especially\\nmodal when the difference is large, such as images and text.\\nThe modalities of the data set AWA are all derived from\\nlanguage. Using the stitched feature vectors, both the SVM\\nclassifier and the PCA + SVM classification can achieve\\nhigher classification accuracy than using a single feature. The\\nclassification accuracy obtained using multi-core learning\\nmethods is rather low. The classification accuracy obtained\\nby the model proposed in this paper is still the highest among\\nseveral methods. The experimental structures on both datasets\\nVOLUME 8, 2020 46339\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 3. Changes in classification accuracy using fusion features of\\ndifferent dimensions on different datasets.\\nshow that the multimodal fusion features extracted using the\\nproposed model have strong discriminative ability.\\nThe obtained discriminative ability of the fused features is\\nan important criterion for measuring whether the algorithm\\ncan effectively integrate multiple modalities. In addition to\\ngood discriminative ability, a good feature should also save\\nas much information as possible in the original data in the\\nlowest possible dimension. Therefore, this paper also tested\\nthe model’s feature dimensionality reduction capability on\\nthe original multimodal data. The accuracy results are shown\\nin Figure 3.\\nIn many existing deep learning structures, only the discriminative loss of the model is considered, and the model\\nproposed in this paper considers both discriminative loss\\nand generation loss. Therefore, this paper also compares the\\nmodel classification performance when considering only discriminative loss and considering both discriminative loss and\\ngeneration loss. In Figure 3, the model using only discriminant loss is represented as Ldis, and the green line segment\\nis used; the model that considers discriminative loss and\\ngeneration loss is represented as Ldis + Lgen, and the blue\\nline segment is used.\\nObviously, the model proposed in this paper can extract\\nlow-dimensional features from the original high-dimensional\\nfeatures, effectively integrate a variety of original features,\\nand have a lower dimension while maintaining strong discriminative ability. It can be seen that the introduction of\\nthe generation loss Lgen makes the features extracted by the\\nmodel have a stronger discriminative ability under the same\\nFIGURE 4. Model test classification accuracy when different modalities\\nare missing.\\nfeature dimensions, which also verifies the validity of the\\nmodel again.\\nIn order to verify the ability of the proposed model to discover the implicit connections between different modalities,\\nthis paper also conducted a set of verification experiments in\\nthe absence of modalities to verify the performance of the\\nmodel in the absence of modalities. In the model training\\nphase, all modals are used for training, and when testing the\\ndata set, one of the modals is removed in order to observe\\nwhether the multi-modal fusion features extracted from the\\nremaining modals using the trained model are still valid. The\\nexperimental results are shown in Figure 4, where U + L\\nindicates that all labeled and unlabeled data are used during\\nmodel training, and L indicates that only labeled data is used\\nduring model training. ‘‘None’’ indicates that all data modalities are used. The experimental results show that if one of the\\nmodalities is missing during the test, the features extracted\\nfrom the remaining modalities can still obtain valid features,\\nand the feature discrimination ability obtained when different\\nmodalities are missing is only slightly lost. At the same time,\\nit was found that using unsupervised data for model training\\ncan improve the discriminative ability of the model to extract\\nfeatures. The decrease in classification accuracy when the\\nmodal is missing is more moderate and the accuracy decline\\nis smaller than using only labeled data. The accuracy of the\\npairwise classification of the dataset in different layers is\\nshown in Figure 5.\\nIII. ENGLISH WORD SEGMENTATION PROCESSING\\nBASED ON BI-GRU-CRF MIXED NETWORK\\nA. SEGMENTATION PROCESSING FRAMEWORK BASED\\nON NEURAL NETWORK\\nThe character-based sequence labeling task can be similarly\\nregarded as an English word segmentation task. Specifically,\\n46340 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 5. Classification accuracy of the dataset at different layers of the\\nproposed model.\\ngiven a mark for each character in the input sentence, during\\nthe word segmentation processing phase, these marks can be\\nused to identify the position of the word in the sentence.\\nIn the research of word segmentation processing based on\\ndeep learning methods, the commonly used annotation set\\nfor researchers is the four-position (B, M, E, S) annotation\\nset, where B represents the beginning of the sentence and M\\nrepresents the middle, E is the ending participle, and S is the\\nparticiple composed of individual words. The other tag set is a\\nsix-position (B, B1, B2, M, E, S) tag set, where B1 and B2 are\\nused to finely divide characters and more effectively label\\nthe position of the characters in the sentence. English word\\nsegmentation framework based on neural network generally\\nincludes three parts: (1) the character embedding layer, that\\nis, the input English characters are converted into a vector\\nmatrix through Word Embedding; (2) a conversion layer with\\nmultiple neural networks; (3) a labeling discrimination layer,\\nwhich is used for subsequent discrimination, and each word\\nmarks its own tag information.\\nWe open a context window for each character in the\\nsentence, which is a commonly used method of character\\nmarking, and then use other character characteristics in the\\nwindow to determine the category mark of the character.\\nGiven a sentence c (n) of input length n, it sets the size of the\\nwindow to w, and the starting character c (1) slides through\\nthe window to the end character c (n). The specific steps of\\nneural network English word segmentation are as follows:\\n(1) Let zi be the linear conversion result from the input\\nlayer to the hidden layer, and its expression is:\\nzi = w1 × xi + b1 (6)\\nIn the above formula, w1 is the weight matrix and b1 is the\\ndeviation coefficient.\\n(2) We pass the linear transformation result through the\\nelement-level activation function σ to obtain the hidden layer\\nfunction. The expression is:\\nhi = σ(zi) (7)\\n(3) Using the given set of labels, we use the linear transformation method to do the final linear transformation to get\\nthe probability yi of the current input character being labeled.\\nThe linear expression is:\\nyi = w2 × xi − b2 (8)\\nIn the above formula, w2 is the weight matrix and b2 is the\\ndeviation coefficient.\\nB. WORD SEGMENTATION PROCESSING BASED ON\\nBI-GRU-CRF HYBRID NETWORK\\nAlthough the BI-LSTM network model has achieved excellent results in the accuracy of English word segmentation processing, the internal structure of the model is complex and the\\ntime cost of processing data is high. Therefore, this paper uses\\nthe BI-GRU neural network to reduce the time cost under the\\npremise of ensuring that the accuracy of the word processing\\nis close to that of the BI-LSTM network. Combined with\\nthe CRF model, a word segmentation model based on the\\nBI-GRU-CRF hybrid network is proposed. This model can\\nnot only cost less time, but also retains the function of BILSTM to resolve long-distance dependencies, and also uses\\nthe CRF layer to efficiently consider the label information\\nbefore and after the sentence. The model structure diagram is\\nshown in Figure 6.\\nLet the newly introduced state transition matrix be A, and\\nthe output of the double-layer GRU neural network be matrix\\nP. Where Ai,j represents the weight transferred from label i to\\nlabel j in time series. If the value of Ai,j\\nis greater, it means\\nthat the probability of transfer from label i to label j is greater;\\nPi,j\\nis the input observation sequence. The i-th word is the\\nprobability of the j-th label. It can be concluded that the\\nprediction output of the labeled sequence y = (y1, y2,. . . , yn)\\ncorresponding to the observation sequence T = (t1, t2,\\n. . . , tn)\\ncan be expressed as:\\nt(T , y) =\\nXn\\ni=1\\n(A + Pi,y1\\n) (9)\\nC. SIMULATION EXPERIMENT AND ANALYSIS\\nTRAINING METHOD AND EXPERIMENTAL STEPS USING\\nTHE MODEL\\nIn the process of English word segmentation, what we have to\\nsolve is how to correctly label each word in the input text so\\nthat the required vocabulary can be identified according to the\\nlabel in the subsequent processing. This paper uses the sixposition tagging method to mark the text separately and uses\\nthe dropout method to avoid overfitting problems in neural\\nnetworks.\\nVOLUME 8, 2020 46341\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 6. Schematic diagram of the word segmentation structure of\\nBI-GRU-CRF hybrid network.\\nThe steps of BI-GRU-CRF-based English word segmentation processing method proposed in this paper are as follows:\\n(1) We input the question sentence into the word embedding layer, and perform word vector conversion through the\\nmethod of word2vec, so that each word is transformed into a\\nspace vector with a length of d;\\n(2) We use the two-way gate recurrent network to\\nautonomously learn the space vector obtained by the previous\\nconversion, obtain the positive and negative semantic features\\nin the vector, adjust the weight and deviation coefficients\\nin the network, and finally obtain the feature vector of the\\nproblem;\\n(3) Probability calculation is performed on the feature\\nvector through the Softmax layer to obtain the part-of-speech\\nprobability distribution of words;\\n(4) We input the part-of-speech probability distribution of\\nthe words into the conditional random field layer, perform\\nsequence labeling at the sentence level, and obtain the final\\nword segmentation processing result.\\nEXPERIMENTAL DATA AND PARAMETER SETTINGS\\nThe data used in the experiment are from three corpus\\ndatabases: SQuAD, Wiki Text, and NarrativeQA. During the\\nexperiment, this data set is divided into three categories:\\ntraining data set, development data set, and test data set. Since\\nthe SQuAD and Wiki Text datasets directly contain test sets,\\nthe test sets provided by them are directly adopted. 10% of the\\ntraining set is randomly selected as the development set, and\\nthe rest of the training set is used as the training set for this\\nexperiment. All data are pre-processed before training on the\\ninput data set. Idioms, consecutive English words or numbers\\nin the data set are replaced with ∗. In order to ensure the\\nFIGURE 7. Comparison results of different models on the SQuAD, Wiki\\nText, and NarrativeQA test sets.\\nreliability of the experimental comparison, the parameters of\\nthe experimental environment and the neural network are set\\nuniformly in this paper. The same parameters are used for\\nall network models. The specific parameter data is shown\\nin Table 2.\\nEXPERIMENTAL RESULTS AND ANALYSIS\\nThis article conducts comparative experiments from two\\naspects of accuracy and time consumption. In terms of accuracy, the BI-GRU-CRF network model proposed in this paper\\nis compared with the CRF, LSTM, BI-LSTM, GRU, and\\nBI-LSTM-CRF network models. The comparison results are\\nshown in Figure 7.\\nIt can be concluded from Figure 7 (a) that the test accuracy of the proposed BI-GRU-CRF hybrid network word\\nsegmentation method is higher than that of the CRF logarithmic linear model, which shows that the method in this\\npaper has outstanding word segmentation. In the remaining\\ngroups of neural network models, they are able to solve\\nthe long-distance dependence of text information. From the\\n46342 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nTABLE 2. Experimental environment parameters and network parameter\\nsettings.\\nTABLE 3. Single sentence average training time.\\nTABLE 4. Average prediction time of a single sentence.\\nexperimental results in Figures 7 (b) and 7 (c), it can be\\nconcluded that the results of the two-way neural network\\nmodel are superior to the single-term network model, such as\\nthe F1 value of BI-LSTM is higher than the F1 value of LSTM\\nand GRU This also shows that the two-way neural network is\\nbetter at understanding and saving text information than the\\none-way neural network model.\\nIt can be seen from Table 3 and Table 4, the BI-GRU-CRF\\nneural network proposed in this paper reduces the sentence\\ntraining time and prediction time consumption of the data\\nset, and the time cost is lower than BI-LSTM- CRF neural\\nnetwork.\\nIt is obvious from Figures 8, training time for each of the\\n500 sentences, The training time of BI-LSTM-CRF is higher\\nthan BI-GRU-CRF. On the other hand, the results of predicted\\ntime of each of the 500 sentences as shown in Figure 9, we can\\nsee that the prediction time of BI-LSTM-CRF is higher than\\nBI-GRU-CRF. From the above results, we can see that the\\npredicted results are relatively close to the actual results.\\nFrom the training and prediction of a single sentence in the\\nabove three data sets, it can be obtained from Table 3 and\\nTable 4 that the network proposed in this paper is 1.62 of\\nthe BI-LSTM-CRF network in training speed. The average\\nFIGURE 8. Training time for each of the 500 sentences.\\nFIGURE 9. The predicted time of each of the 500 sentences.\\nFIGURE 10. Loss curve of BI-LSTM-CRF network and BI-GRU-CRF network.\\nspeed of making predictions on the test set is 1.94 times\\nthat of the BI-LSTM-CRF network. The reason for these\\nhuge differences is that the core network elements of the two\\nVOLUME 8, 2020 46343\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nmodels are different. The model proposed in this paper is\\nbased on the GRU neural network, and the GRU replaces\\nthe input and output gates in the LSTM with an update gate,\\ndiscards the complicated memory cells in the LSTM, and uses\\nthe reset gate to control the input of hidden node information.\\nThis simplification allows the GRU to perform two non-linear\\ntransformation calculations each time compared to the LSTM\\nfor hidden node calculations. In a two-way neural network,\\nthe GRU performs four fewer than the LSTMs for each hidden\\nnode calculation. The faster the training and prediction, the\\nless time cost. The loss curves of the BI-LSTM-CRF network\\nand BI-GRU-CRF network are shown in Figure 10. It is\\nobvious from Figure 10 that the Loss of the BI-GRU-CRF\\nnetwork is generally smaller than that of the BI-LSTM-CRF\\nnetwork.\\nIV. CONCLUSION\\nThis paper proposes a multimodal shared feature expression extraction algorithm based on deep neural network,\\ngives the entire model structure of the algorithm, and details\\nthe design of the model structure and the model training\\nmethod. In order to verify the effectiveness of the proposed\\nmodel, a series of comparative experiments were carried out.\\nThe experimental results show that the proposed multimodal\\nfusion feature extraction model can effectively extract lowdimensional fusion features from the original multiple highdimensional data. The obtained fusion feature expression has\\na strong discriminative ability while possessing a lower feature dimension, thereby proving the validity of the proposed\\nmodel. In terms of English word segmentation, this article\\nhas studied LSTM and GRU in depth. After analysis and\\nresearch, both networks can solve the problem of traditional\\nword segmentation in the long-range dependency relationship\\nof text. However, due to the complexity of its structure,\\nLSTM consumes a lot of time in the process of training and\\npredicting the data set. The GRU is a simplified version of\\nthe LSTM. It has a simple structure and consumes less time\\nin training and prediction. Based on the two-way network’s\\nability to better capture the contextual relationship between\\nsemantics, this paper combines BI-GRU and CRF models,\\nand proposes a hybrid neural network word segmentation\\nprocessing method. The experimental results show that the\\nmodel proposed in this paper is better than most previous\\nmodels in terms of accuracy, and in terms of timeliness, the\\nmethod proposed in this paper is 1.62 times faster than the BILSTM-CRF network word segmentation method in training\\nspeed. The average speed is 1.94 times that of the word segmentation method based on BI-LSTM-CRF network. Based\\non these two sets of data, the hybrid network word segmentation method proposed in this paper has good performance in\\nEnglish word segmentation. In future work, we can consider\\nanalyzing the impact of different feature extraction methods\\nand feature selection methods on the model, thereby further\\nenhancing the learning ability of the model. The proposed\\nmethod treats different features obtained by different extraction methods in each kind of raw data as an independent\\nmode, and does not learn directly on the raw data. How to\\nget the multi-modal fusion low-dimensional features directly\\nfrom the original multi-modal data needs further research.\\nREFERENCES\\n[1] V. K. Ha, J.-C. Ren, and X.-Y. Xu, ‘‘Deep learning based single image\\nsuper-resolution: A survey,’’ Int. J. Autom. Comput., vol. 16, no. 4,\\npp. 413–426, 2019.\\n[2] F. Meng, P. Chen, L. Wu, and X. Wang, ‘‘Automatic modulation classification: A deep learning enabled approach,’’ IEEE Trans. Veh. Technol.,\\nvol. 67, no. 11, pp. 10760–10772, Nov. 2018.\\n[3] Q. Xia, S. Li, A. M. Hao, and Q. P. Zhao, ‘‘Deep learning for digital\\ngeometry processing and analysis: A review,’’ J. Comput. Res. Develop.,\\nvol. 56, no. 1, pp. 155–182, 2019.\\n[4] Y. Chen, Y. Zhang, S. Maharjan, M. Alam, and T. Wu, ‘‘Deep learning for\\nsecure mobile edge computing in cyber-physical transportation systems,’’\\nIEEE Netw., vol. 33, no. 4, pp. 36–41, Jul. 2019.\\n[5] K. A. Weber, A. C. Smith, M. Wasielewski, K. Eghtesad,\\nP. A. Upadhyayula, M. Wintermark, T. J. Hastie, T. B. Parrish, S. Mackey,\\nand J. M. Elliott, ‘‘Deep learning convolutional neural networks for the\\nautomatic quantification of muscle fat infiltration following whiplash\\ninjury,’’ Sci. Rep., vol. 9, no. 1, p. 7973, May 2019.\\n[6] O. Bernard et al., ‘‘Deep learning techniques for automatic MRI cardiac\\nmulti-structures segmentation and diagnosis: Is the problem solved,’’ IEEE\\nTrans. Med. Imag., vol. 37, no. 11, pp. 2514–2525, Nov. 2018.\\n[7] M. Abdughani, J. Ren, L. Wu, J.-M. Yang, and J. Zhao, ‘‘Supervised deep\\nlearning in high energy phenomenology: A mini review,’’ Commun. Theor.\\nPhys., vol. 71, no. 8, p. 955, Aug. 2019.\\n[8] R. Ranjan, S. Sankaranarayanan, A. Bansal, N. Bodla, J.-C. Chen,\\nV. M. Patel, C. D. Castillo, and R. Chellappa, ‘‘Deep learning for understanding faces: Machines may be just as good, or better, than humans,’’\\nIEEE Signal Process. Mag., vol. 35, no. 1, pp. 66–83, Jan. 2018.\\n[9] Y. Tian and X. Liu, ‘‘A deep adaptive learning method for rolling bearing\\nfault diagnosis using immunity,’’ Tsinghua Sci. Technol., vol. 24, no. 6,\\npp. 750–762, Dec. 2019.\\n[10] C. Wu, R. Zeng, J. Pan, C. C. L. Wang, and Y.-J. Liu, ‘‘Plant phenotyping by\\ndeep-learning-based planner for multi-robots,’’ IEEE Robot. Autom. Lett.,\\nvol. 4, no. 4, pp. 3113–3120, Oct. 2019.\\n[11] H. Gao, W. Huang, and X. Yang, ‘‘Applying probabilistic model checking to path planning in an intelligent transportation system using mobility trajectories and their statistical data,’’ Intell. Automat. Soft Comput.\\n(Autosoft), vol. 25, no. 3, pp. 547–559, 2019.\\n[12] H. Gao, W. Huang, Y. Duan, X. Yang, and Q. Zou, ‘‘Research on costdriven services composition in an uncertain environment,’’ J. Internet\\nTechnol. (JIT), vol. 20, no. 3, pp. 755–769, 2019.\\n[13] H. Gao, Y. Duan, L. Shao, and X. Sun, ‘‘Transformation-based processing of typed resources for multimedia sources in the IoT environment,’’\\nWireless Netw., Nov. 2019, pp. 1–17, doi: 10.1007/s11276-019-02200-6.\\n[14] H. Gao, Y. Xu, Y. Yin, W. Zhang, R. Li, and X. Wang, ‘‘Contextaware QoS prediction with neural collaborative filtering for Internetof-Things services,’’ IEEE Internet Things J., to be published, doi:\\n10.1109/JIOT.2019.2956827.\\n[15] X. Ma, H. Gao, H. Xu, and M. Bian, ‘‘An IoT-based task scheduling\\noptimization scheme considering the deadline and cost-aware scientific\\nworkflow for cloud computing,’’ EURASIP J. Wireless Commun. Netw.,\\nvol. 1, p. 249, Dec. 2019, doi: 10.1186/s13638-019-1557-3.\\n[16] S. Nie, M. Zheng, and Q. Ji, ‘‘The deep regression Bayesian network and\\nits applications: Probabilistic deep learning for computer vision,’’ IEEE\\nSignal Process. Mag., vol. 35, no. 1, pp. 101–111, Jan. 2018.\\n[17] A. Kumar, S. Mukherjee, and A. K. Luhach, ‘‘Deep learning with perspective modeling for early detection of malignancy in mammograms,’’\\nJ. Discrete Math. Sci. Cryptogr., vol. 22, no. 4, pp. 627–643, Sep. 2019.\\n[18] Z. Yan, X. Yang, and K.-T. Cheng, ‘‘Joint segment-level and pixel-wise\\nlosses for deep learning based retinal vessel segmentation,’’ IEEE Trans.\\nBiomed. Eng., vol. 65, no. 9, pp. 1912–1923, Sep. 2018.\\n[19] M. Treder, J. L. Lauermann, M. Alnawaiseh, and N. Eter, ‘‘Using deep\\nlearning in automated detection of graft detachment in descemet membrane\\nendothelial keratoplasty: A pilot study,’’ Cornea, vol. 38, no. 2, p. 1, 2018.\\n[20] P. Zhou, X. Fang, X. Wang, Y. Long, R. He, and X. Han, ‘‘Deep learningbased beam management and interference coordination in dense mmWave\\nnetworks,’’ IEEE Trans. Veh. Technol., vol. 68, no. 1, pp. 592–603,\\nJan. 2019.\\n46344 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\n[21] J. Yu, C. Zhu, J. Zhang, Q. Huang, and D. Tao, ‘‘Spatial pyramid-enhanced\\nNetVLAD with weighted triplet loss for place recognition,’’ IEEE Trans.\\nNeural Netw. Learn. Syst., vol. 31, no. 2, pp. 661–674, Feb. 2020, doi:\\n10.1109/TNNLS.2019.2908982.\\n[22] J. Yu, Z. Kuang, B. Zhang, W. Zhang, D. Lin, and J. Fan, ‘‘Leveraging\\ncontent sensitiveness and user trustworthiness to recommend fine-grained\\nprivacy settings for social image sharing,’’ IEEE Trans. Inf. Forensics\\nSecur., vol. 13, no. 5, pp. 1317–1332, May 2018.\\n[23] J. Yu, C. Hong, Y. Rui, and D. Tao, ‘‘Multitask Autoencoder model for\\nrecovering human poses,’’ IEEE Trans. Ind. Electron., vol. 65, no. 6,\\npp. 5060–5068, Jun. 2018.\\n[24] H. Masumoto, H. Tabuchi, S. Nakakura, N. Ishitobi, M. Miki, and H. Enno,\\n‘‘Deep-learning classifier with an ultrawide-field scanning laser ophthalmoscope detects glaucoma visual field severity,’’ J. Glaucoma, vol. 27,\\nno. 7, pp. 647–652, Jul. 2018.\\n[25] J. A. K. Suykens, ‘‘Deep restricted kernel machines using conjugate feature\\nduality,’’ Neural Comput., vol. 29, no. 8, pp. 2123–2163, Aug. 2017.\\n[26] Y. Yin, J. Xia, Y. Li, Y. Xu, W. Xu, and L. Yu, ‘‘Group-wise itinerary\\nplanning in temporary mobile social network,’’ IEEE Access, vol. 7,\\npp. 83682–83693, 2019.\\n[27] Y. Yin, L. Chen, Y. Xu, J. Wan, H. Zhang, and Z. Mai, ‘‘QoS prediction\\nfor service recommendation with deep feature learning in edge computing\\nenvironment,’’ Mobile Netw. Appl., Apr. 2019, doi: 10.1007/s11036-019-\\n01241-7.\\n[28] Y. Chen, S. Deng, and H. Ma, and J. Yin, ‘‘Deploying data-intensive\\napplications with multiple services components on edge,’’ Mobile Netw.\\nAppl., pp. 1–16, Apr. 2019, doi: 10.1007/s11036-019-01245-3.\\n[29] C. Zhang, H. Zhao, and S. Deng, ‘‘A density-based offloading strategy for IoT devices in edge computing systems,’’ IEEE Access, vol. 6,\\npp. 73520–73530, Nov. 2018.\\n[30] L. Kuang, X. Yan, X. Tan, S. Li, and X. Yang, ‘‘Predicting taxi demand\\nbased on 3D convolutional neural network and multi-task learning,’’\\nRemote Sens., vol. 11, no. 11, p. 1265, May 2019.\\n[31] M. Koohzadi and N. M. Charkari, ‘‘Survey on deep learning methods in\\nhuman action recognition,’’ IET Comput. Vis., vol. 11, no. 8, pp. 623–632,\\nDec. 2017.\\n[32] G. Guo and A. Lai, ‘‘A survey on still image based human action recognition,’’ Pattern Recognit., vol. 47, no. 10, pp. 3343–3361, Oct. 2014.\\n[33] J. Zhu, Q. Zhou, W. Zou, R. Zhang, and W. Zhang, ‘‘A generalized pyramid\\nmatching kernel for human action recognition in realistic videos,’’ Sensors,\\nvol. 13, no. 11, pp. 14398–14416, Oct. 2013.\\n[34] X. B. Peng, G. Berseth, K. Yin, and M. Van De Panne, ‘‘DeepLoco:\\nDynamic locomotion skills using hierarchical deep reinforcement learning,’’ ACM Trans. Graph., vol. 36, no. 4, pp. 1–13, Jul. 2017.\\n[35] L. Liu and J. Hodgins, ‘‘Learning to schedule control fragments for\\nphysics-based characters using deep Q-learning,’’ ACM Trans. Graph.,\\nvol. 36, no. 4, p. 1, Jul. 2017.\\n[36] M. Gharbi, J. Chen, J. T. Barron, S. W. Hasinoff, and F. Durand, ‘‘Deep\\nbilateral learning for real-time image enhancement,’’ ACM Trans. Graph.,\\nvol. 36, no. 4, pp. 1–12, Jul. 2017.\\n[37] S. Doan, L. Bastarache, S. Klimkowski, J. C. Denny, and H. Xu, ‘‘Integrating existing natural language processing tools for medication extraction\\nfrom discharge summaries,’’ J. Amer. Med. Inform. Assoc., vol. 17, no. 5,\\npp. 528–531, Sep. 2010.\\n[38] T. Merabti, M. Joubert, T. Lecroq, A. Rath, and S. J. Darmoni, ‘‘Mapping\\nbiomedical terminologies using natural language processing tools and\\nUMLS: Mapping the orphanet thesaurus to the MeSH,’’ IRBM, vol. 31,\\nno. 4, pp. 221–225, Sep. 2010.\\n[39] L. S. Anderson, H. G. Bell, and M. Gilbert, ‘‘Using social listening data\\nto monitor misuse and nonmedical use of Bupropion: A content analysis,’’\\nJMIR Public Health Surveill., vol. 3, no. 1, p. e6, 2017.\\n[40] A. P. Chaudhry, N. Afzal, M. M. Abidian, V. P. Mallipeddi, R. K. Elayavilli,\\nC. G. Scott, I. J. Kullo, P. W. Wennberg, J. J. Pankratz, H. Liu, R. Chaudhry,\\nand A. M. Arruda-Olson, ‘‘Innovative informatics approaches for peripheral artery disease: Current state and provider survey of strategies for\\nimproving guideline-based care,’’ Mayo Clin Proc., Innov. Qual. Outcomes, vol. 2, no. 2, pp. 129–136, Jun. 2018.\\n[41] M. E. Tibbo, C. C. Wyles, S. Fu, S. Sohn, D. G. Lewallen, D. J. Berry, and\\nH. Maradit Kremers, ‘‘Use of natural language processing tools to identify and classify periprosthetic femur fractures,’’ J. Arthroplasty, vol. 34,\\nno. 10, pp. 2216–2219, Oct. 2019.\\n[42] A. M. Reza, ‘‘Realization of the contrast limited adaptive histogram\\nequalization (CLAHE) for real-time image enhancement,’’ J. VLSI Signal\\nProcess. Syst. Signal, Image, Video Technol., vol. 38, no. 1, pp. 35–44,\\nAug. 2004.\\n[43] M.-C. Chen, S.-Q. Lu, and Q.-L. Liu, ‘‘Global regularity for a 2D model of\\nelectro-kinetic fluid in a bounded domain,’’ Acta Mathematicae Applicatae\\nSinica, English Ser., vol. 34, no. 2, pp. 398–403, Apr. 2018.\\nDONGYANG WANG was born in Henan, China,\\nin 1983. He received the bachelor’s degree from\\nHenan University, in 2006, and the master’s degree\\nfrom Shaanxi Normal University, in 2014. From\\n2007 to 2011, he worked at the New Oriental\\nSchool. Since 2019, he has been studying with the\\nLyceum of the Philippines University-Batangas.\\nHe has published eight articles. His research interest concentrates on English language studies.\\nJUNLI SU was born in Henan, China, in 1984. She\\nreceived the bachelor’s and master’s degrees from\\nHenan University, in 2006 and 2009, respectively.\\nSince 2009, she has been working with the Jiaozuo\\nTeachers College. She has published more than\\n15 articles, responsible for four scientific research\\nprojects. Her research interests mainly focus on\\npedagogy and psychology.\\nHONGBIN YU received the Ph.D. degree in computer science and technology from Shanghai Jiao\\nTong University, in 2016, and the M.S. degree in\\npattern recognition and intelligent system from the\\nUniversity of Science and Technology of China,\\nin 2009. He is currently a Lecturer with the\\nSchool of Digital Media, Jiangnan University. His\\nresearch interests include brain–computer interface and computer vision.\\nVOLUME 8, 2020 46345', doc_id='963674de-2cdd-490c-b97a-0f32d2bdae89', embedding=None, doc_hash='dc80581419b4c9c28db7a9aea5b2babd8f800f82920bcfd9a680a7ed685d5fab', extra_info={'catgeory': 'NLP', 'filename': 'NLP'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3fae1d5-b829-40be-b79f-a6ff5bf7d9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents_with_extra_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "421d3b9e-90cf-4b6b-8eb6-80a265e7a402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "index = ListIndex.from_documents(documents_with_extra_info)\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"<query_text>\")\n",
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f482ec1-c375-4e6f-bf35-e7db4e47e502",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# documents_texts= []  # List of 'Document' objects\n",
    "\n",
    "# # Extract the necessary information from each 'Document' object\n",
    "# # documents = [Document(t) for t in text_list]\n",
    "# document_texts = [Document(doc) for doc in documents_with_extra_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d03be24c-778b-4211-b565-9c5b825aaa2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3930dcc-c4af-4b6f-ae3e-79558afe2150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ListIndex from the extracted text\n",
    "index = GPTVectorStoreIndex.from_documents(documents_with_extra_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dede986e-88f7-4496-b859-7a363cec5952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "649c7b9d-8faa-4318-9d27-40a5390c461d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0d65d6b-bc39-493d-8aad-3d358f1280db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daf15e23-d2bd-435c-813e-fc13a8774628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "513bc163-793c-4c78-8885-b7a303980b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nBERT is a language representation model that is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. It is conceptually simple and empirically powerful, obtaining new state-of-the-art results on eleven natural language processing tasks. BERT is able to push the GLUE score to 80.5%, MultiNLI accuracy to 86.7%, SQuAD v1.1 question answering Test F1 to 93.2, and SQuAD v2.0 Test F1 to 83.1.', source_nodes=[NodeWithScore(node=Node(text='model, a word segmentation model based on the\\nBI-GRU-CRF hybrid network is proposed. This model can\\nnot only cost less time, but also retains the function of BILSTM to resolve long-distance dependencies, and also uses\\nthe CRF layer to efficiently consider the label information\\nbefore and after the sentence. The model structure diagram is\\nshown in Figure 6.\\nLet the newly introduced state transition matrix be A, and\\nthe output of the double-layer GRU neural network be matrix\\nP. Where Ai,j represents the weight transferred from label i to\\nlabel j in time series. If the value of Ai,j\\nis greater, it means\\nthat the probability of transfer from label i to label j is greater;\\nPi,j\\nis the input observation sequence. The i-th word is the\\nprobability of the j-th label. It can be concluded that the\\nprediction output of the labeled sequence y = (y1, y2,. . . , yn)\\ncorresponding to the observation sequence T = (t1, t2,\\n. . . , tn)\\ncan be expressed as:\\nt(T , y) =\\nXn\\ni=1\\n(A + Pi,y1\\n) (9)\\nC. SIMULATION EXPERIMENT AND ANALYSIS\\nTRAINING METHOD AND EXPERIMENTAL STEPS USING\\nTHE MODEL\\nIn the process of English word segmentation, what we have to\\nsolve is how to correctly label each word in the input text so\\nthat the required vocabulary can be identified according to the\\nlabel in the subsequent processing. This paper uses the sixposition tagging method to mark the text separately and uses\\nthe dropout method to avoid overfitting problems in neural\\nnetworks.\\nVOLUME 8, 2020 46341\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 6. Schematic diagram of the word segmentation structure of\\nBI-GRU-CRF hybrid network.\\nThe steps of BI-GRU-CRF-based English word segmentation processing method proposed in this paper are as follows:\\n(1) We input the question sentence into the word embedding layer, and perform word vector conversion through the\\nmethod of word2vec, so that each word is transformed into a\\nspace vector with a length of d;\\n(2) We use the two-way gate recurrent network to\\nautonomously learn the space vector obtained by the previous\\nconversion, obtain the positive and negative semantic features\\nin the vector, adjust the weight and deviation coefficients\\nin the network, and finally obtain the feature vector of the\\nproblem;\\n(3) Probability calculation is performed on the feature\\nvector through the Softmax layer to obtain the part-of-speech\\nprobability distribution of words;\\n(4) We input the part-of-speech probability distribution of\\nthe words into the conditional random field layer, perform\\nsequence labeling at the sentence level, and obtain the final\\nword segmentation processing result.\\nEXPERIMENTAL DATA AND PARAMETER SETTINGS\\nThe data used in the experiment are from three corpus\\ndatabases: SQuAD, Wiki Text, and NarrativeQA. During the\\nexperiment, this data set is divided into three categories:\\ntraining data set, development data set, and test data set. Since\\nthe SQuAD and Wiki Text datasets directly contain test sets,\\nthe test sets provided by them are directly adopted. 10% of the\\ntraining set is randomly selected as the development set, and\\nthe rest of the training set is used as the training set for this\\nexperiment. All data are pre-processed before training on the\\ninput data set. Idioms, consecutive English words or numbers\\nin the data set are replaced with ∗. In order to ensure the\\nFIGURE 7. Comparison results of different models on the SQuAD, Wiki\\nText, and NarrativeQA test sets.\\nreliability of the experimental comparison, the parameters of\\nthe experimental environment and the neural network are set\\nuniformly in this paper. The same parameters are used for\\nall network models. The specific parameter data is shown\\nin Table', doc_id='709e1cb1-a777-4b50-a657-8967d615c990', embedding=None, doc_hash='ff35b589f2f19e56d3717ca70e1505d895943407d7ff01ad549218bcfb11ce72', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 32125, 'end': 35839, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', <DocumentRelationship.NEXT: '3'>: 'e47306bd-0a13-472b-b254-a0872d3e2fa7'}), score=0.8610769670797819), NodeWithScore(node=Node(text='valid features,\\nand the feature discrimination ability obtained when different\\nmodalities are missing is only slightly lost. At the same time,\\nit was found that using unsupervised data for model training\\ncan improve the discriminative ability of the model to extract\\nfeatures. The decrease in classification accuracy when the\\nmodal is missing is more moderate and the accuracy decline\\nis smaller than using only labeled data. The accuracy of the\\npairwise classification of the dataset in different layers is\\nshown in Figure 5.\\nIII. ENGLISH WORD SEGMENTATION PROCESSING\\nBASED ON BI-GRU-CRF MIXED NETWORK\\nA. SEGMENTATION PROCESSING FRAMEWORK BASED\\nON NEURAL NETWORK\\nThe character-based sequence labeling task can be similarly\\nregarded as an English word segmentation task. Specifically,\\n46340 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 5. Classification accuracy of the dataset at different layers of the\\nproposed model.\\ngiven a mark for each character in the input sentence, during\\nthe word segmentation processing phase, these marks can be\\nused to identify the position of the word in the sentence.\\nIn the research of word segmentation processing based on\\ndeep learning methods, the commonly used annotation set\\nfor researchers is the four-position (B, M, E, S) annotation\\nset, where B represents the beginning of the sentence and M\\nrepresents the middle, E is the ending participle, and S is the\\nparticiple composed of individual words. The other tag set is a\\nsix-position (B, B1, B2, M, E, S) tag set, where B1 and B2 are\\nused to finely divide characters and more effectively label\\nthe position of the characters in the sentence. English word\\nsegmentation framework based on neural network generally\\nincludes three parts: (1) the character embedding layer, that\\nis, the input English characters are converted into a vector\\nmatrix through Word Embedding; (2) a conversion layer with\\nmultiple neural networks; (3) a labeling discrimination layer,\\nwhich is used for subsequent discrimination, and each word\\nmarks its own tag information.\\nWe open a context window for each character in the\\nsentence, which is a commonly used method of character\\nmarking, and then use other character characteristics in the\\nwindow to determine the category mark of the character.\\nGiven a sentence c (n) of input length n, it sets the size of the\\nwindow to w, and the starting character c (1) slides through\\nthe window to the end character c (n). The specific steps of\\nneural network English word segmentation are as follows:\\n(1) Let zi be the linear conversion result from the input\\nlayer to the hidden layer, and its expression is:\\nzi = w1 × xi + b1 (6)\\nIn the above formula, w1 is the weight matrix and b1 is the\\ndeviation coefficient.\\n(2) We pass the linear transformation result through the\\nelement-level activation function σ to obtain the hidden layer\\nfunction. The expression is:\\nhi = σ(zi) (7)\\n(3) Using the given set of labels, we use the linear transformation method to do the final linear transformation to get\\nthe probability yi of the current input character being labeled.\\nThe linear expression is:\\nyi = w2 × xi − b2 (8)\\nIn the above formula, w2 is the weight matrix and b2 is the\\ndeviation coefficient.\\nB. WORD SEGMENTATION PROCESSING BASED ON\\nBI-GRU-CRF HYBRID NETWORK\\nAlthough the BI-LSTM network model has achieved excellent results in the accuracy of English word segmentation processing, the internal structure of the model is complex and the\\ntime cost of processing data is high. Therefore, this paper uses\\nthe BI-GRU neural network to reduce the time cost under the\\npremise of ensuring that the accuracy of the word processing\\nis close to that of the BI-LSTM network. Combined with\\nthe CRF model, a word segmentation model based on the\\nBI-GRU-CRF', doc_id='dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', embedding=None, doc_hash='be96beb267949818af895c16b83ea79fb8e4f04e022b0f18a4c068c20e30900a', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 28259, 'end': 32076, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: '09328148-502b-4307-a672-f67d5921c7a7', <DocumentRelationship.NEXT: '3'>: '709e1cb1-a777-4b50-a657-8967d615c990'}), score=0.8355766782152038)], extra_info={'709e1cb1-a777-4b50-a657-8967d615c990': {'catgeory': 'NLP', 'filename': 'NLP'}, 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4': {'catgeory': 'NLP', 'filename': 'NLP'}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f3d7d22-8944-4cf0-90c3-249758bc5976",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'709e1cb1-a777-4b50-a657-8967d615c990': {'catgeory': 'NLP',\n",
       "  'filename': 'NLP'},\n",
       " 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4': {'catgeory': 'NLP',\n",
       "  'filename': 'NLP'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a8b36acf-39a4-4a9c-a476-6cdf27e256d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# query = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3b421ac-693c-4169-8cf6-f50a00ad799d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'51522920a0934fdbb4567e7640f2e7d3': 'CV',\n",
       " 'd941c28a34934793b61a509ad5c2360c': 'NLP'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776062b7-e0fe-410a-b1b9-9434a3041c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-3.35.2-py3-none-any.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: uvicorn>=0.14.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (0.22.0)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (1.24.3)\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-5.0.1-py3-none-any.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (2.29.0)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (2.15.1)\n",
      "Requirement already satisfied: markupsafe in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (2.1.3)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (3.1.2)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.9/136.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: aiofiles in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (22.1.0)\n",
      "Collecting gradio-client>=0.2.7\n",
      "  Downloading gradio_client-0.2.7-py3-none-any.whl (288 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (5.4.1)\n",
      "Collecting huggingface-hub>=0.14.0\n",
      "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (1.10.8)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fastapi in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (0.96.0)\n",
      "Requirement already satisfied: pillow in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (9.5.0)\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: pandas in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: toolz in /home/ray/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio) (4.5.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: fsspec in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio-client>=0.2.7->gradio) (2023.5.0)\n",
      "Requirement already satisfied: packaging in /home/ray/anaconda3/lib/python3.9/site-packages (from gradio-client>=0.2.7->gradio) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/ray/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: click>=7.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ray/anaconda3/lib/python3.9/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from aiohttp->gradio) (23.1.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: certifi in /home/ray/anaconda3/lib/python3.9/site-packages (from httpx->gradio) (2023.5.7)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /home/ray/anaconda3/lib/python3.9/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: idna in /home/ray/anaconda3/lib/python3.9/site-packages (from httpx->gradio) (3.4)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.40.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from matplotlib->gradio) (5.12.0)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ray/anaconda3/lib/python3.9/site-packages (from requests->gradio) (1.26.15)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->gradio) (3.11.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ray/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ray/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->gradio) (1.13.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/ray/anaconda3/lib/python3.9/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio) (1.1.1)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=7cbae72e263131b8c2629f54c684c09a955fbd8d4eeea0ee6de82549d7a00035\n",
      "  Stored in directory: /home/ray/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
      "Successfully built ffmpy\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f6c0f41f-8919-42db-92a1-6ccc065940fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c1c10-a9bc-4210-afe9-1294641e6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(abstract):\n",
    "    response = query_engine.query(abstract)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f3a7d253-a529-4f24-a120-b530e2f49643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# counter = 1\n",
    "\n",
    "# def generate_output(input_text):\n",
    "#     global counter\n",
    "#     # output_text = \"Hello, \" + input_text + \"!\"\n",
    "#     counter += 1\n",
    "#     # return output_text, gr.Textbox.update(label=f\"Question {counter}\")\n",
    "#     reponse = query_engine.query(input_text)\n",
    "#     return response, gr.Textbox.update(label = f\"Question{counter}\")\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Row():\n",
    "    \n",
    "#         # column for inputs\n",
    "#         with gr.Column():\n",
    "#             abstract_input = gr.Textbox(label=\"Abstract Text\")\n",
    "#             submit_button = gr.Button(\"Submit\")\n",
    "                   \n",
    "#         # column for outputs\n",
    "#         with gr.Column():\n",
    "#             output_text = gr.Textbox()\n",
    "            \n",
    "#     submit_button.click(\n",
    "#         fn=generate_output,\n",
    "#         inputs=abstract_input,\n",
    "#         outputs=[output_text, input_text]\n",
    "#     )\n",
    "\n",
    "# demo.launch(share = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73a56675-663c-423a-9507-a46bfd89dce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='\\nBERT is a language representation model that is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. It is conceptually simple and empirically powerful, obtaining new state-of-the-art results on eleven natural language processing tasks. BERT is able to push the GLUE score to 80.5%, MultiNLI accuracy to 86.7%, SQuAD v1.1 question answering Test F1 to 93.2, and SQuAD v2.0 Test F1 to 83.1.', source_nodes=[NodeWithScore(node=Node(text='model, a word segmentation model based on the\\nBI-GRU-CRF hybrid network is proposed. This model can\\nnot only cost less time, but also retains the function of BILSTM to resolve long-distance dependencies, and also uses\\nthe CRF layer to efficiently consider the label information\\nbefore and after the sentence. The model structure diagram is\\nshown in Figure 6.\\nLet the newly introduced state transition matrix be A, and\\nthe output of the double-layer GRU neural network be matrix\\nP. Where Ai,j represents the weight transferred from label i to\\nlabel j in time series. If the value of Ai,j\\nis greater, it means\\nthat the probability of transfer from label i to label j is greater;\\nPi,j\\nis the input observation sequence. The i-th word is the\\nprobability of the j-th label. It can be concluded that the\\nprediction output of the labeled sequence y = (y1, y2,. . . , yn)\\ncorresponding to the observation sequence T = (t1, t2,\\n. . . , tn)\\ncan be expressed as:\\nt(T , y) =\\nXn\\ni=1\\n(A + Pi,y1\\n) (9)\\nC. SIMULATION EXPERIMENT AND ANALYSIS\\nTRAINING METHOD AND EXPERIMENTAL STEPS USING\\nTHE MODEL\\nIn the process of English word segmentation, what we have to\\nsolve is how to correctly label each word in the input text so\\nthat the required vocabulary can be identified according to the\\nlabel in the subsequent processing. This paper uses the sixposition tagging method to mark the text separately and uses\\nthe dropout method to avoid overfitting problems in neural\\nnetworks.\\nVOLUME 8, 2020 46341\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 6. Schematic diagram of the word segmentation structure of\\nBI-GRU-CRF hybrid network.\\nThe steps of BI-GRU-CRF-based English word segmentation processing method proposed in this paper are as follows:\\n(1) We input the question sentence into the word embedding layer, and perform word vector conversion through the\\nmethod of word2vec, so that each word is transformed into a\\nspace vector with a length of d;\\n(2) We use the two-way gate recurrent network to\\nautonomously learn the space vector obtained by the previous\\nconversion, obtain the positive and negative semantic features\\nin the vector, adjust the weight and deviation coefficients\\nin the network, and finally obtain the feature vector of the\\nproblem;\\n(3) Probability calculation is performed on the feature\\nvector through the Softmax layer to obtain the part-of-speech\\nprobability distribution of words;\\n(4) We input the part-of-speech probability distribution of\\nthe words into the conditional random field layer, perform\\nsequence labeling at the sentence level, and obtain the final\\nword segmentation processing result.\\nEXPERIMENTAL DATA AND PARAMETER SETTINGS\\nThe data used in the experiment are from three corpus\\ndatabases: SQuAD, Wiki Text, and NarrativeQA. During the\\nexperiment, this data set is divided into three categories:\\ntraining data set, development data set, and test data set. Since\\nthe SQuAD and Wiki Text datasets directly contain test sets,\\nthe test sets provided by them are directly adopted. 10% of the\\ntraining set is randomly selected as the development set, and\\nthe rest of the training set is used as the training set for this\\nexperiment. All data are pre-processed before training on the\\ninput data set. Idioms, consecutive English words or numbers\\nin the data set are replaced with ∗. In order to ensure the\\nFIGURE 7. Comparison results of different models on the SQuAD, Wiki\\nText, and NarrativeQA test sets.\\nreliability of the experimental comparison, the parameters of\\nthe experimental environment and the neural network are set\\nuniformly in this paper. The same parameters are used for\\nall network models. The specific parameter data is shown\\nin Table', doc_id='709e1cb1-a777-4b50-a657-8967d615c990', embedding=None, doc_hash='ff35b589f2f19e56d3717ca70e1505d895943407d7ff01ad549218bcfb11ce72', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 32125, 'end': 35839, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', <DocumentRelationship.NEXT: '3'>: 'e47306bd-0a13-472b-b254-a0872d3e2fa7'}), score=0.8610769670797819), NodeWithScore(node=Node(text='valid features,\\nand the feature discrimination ability obtained when different\\nmodalities are missing is only slightly lost. At the same time,\\nit was found that using unsupervised data for model training\\ncan improve the discriminative ability of the model to extract\\nfeatures. The decrease in classification accuracy when the\\nmodal is missing is more moderate and the accuracy decline\\nis smaller than using only labeled data. The accuracy of the\\npairwise classification of the dataset in different layers is\\nshown in Figure 5.\\nIII. ENGLISH WORD SEGMENTATION PROCESSING\\nBASED ON BI-GRU-CRF MIXED NETWORK\\nA. SEGMENTATION PROCESSING FRAMEWORK BASED\\nON NEURAL NETWORK\\nThe character-based sequence labeling task can be similarly\\nregarded as an English word segmentation task. Specifically,\\n46340 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 5. Classification accuracy of the dataset at different layers of the\\nproposed model.\\ngiven a mark for each character in the input sentence, during\\nthe word segmentation processing phase, these marks can be\\nused to identify the position of the word in the sentence.\\nIn the research of word segmentation processing based on\\ndeep learning methods, the commonly used annotation set\\nfor researchers is the four-position (B, M, E, S) annotation\\nset, where B represents the beginning of the sentence and M\\nrepresents the middle, E is the ending participle, and S is the\\nparticiple composed of individual words. The other tag set is a\\nsix-position (B, B1, B2, M, E, S) tag set, where B1 and B2 are\\nused to finely divide characters and more effectively label\\nthe position of the characters in the sentence. English word\\nsegmentation framework based on neural network generally\\nincludes three parts: (1) the character embedding layer, that\\nis, the input English characters are converted into a vector\\nmatrix through Word Embedding; (2) a conversion layer with\\nmultiple neural networks; (3) a labeling discrimination layer,\\nwhich is used for subsequent discrimination, and each word\\nmarks its own tag information.\\nWe open a context window for each character in the\\nsentence, which is a commonly used method of character\\nmarking, and then use other character characteristics in the\\nwindow to determine the category mark of the character.\\nGiven a sentence c (n) of input length n, it sets the size of the\\nwindow to w, and the starting character c (1) slides through\\nthe window to the end character c (n). The specific steps of\\nneural network English word segmentation are as follows:\\n(1) Let zi be the linear conversion result from the input\\nlayer to the hidden layer, and its expression is:\\nzi = w1 × xi + b1 (6)\\nIn the above formula, w1 is the weight matrix and b1 is the\\ndeviation coefficient.\\n(2) We pass the linear transformation result through the\\nelement-level activation function σ to obtain the hidden layer\\nfunction. The expression is:\\nhi = σ(zi) (7)\\n(3) Using the given set of labels, we use the linear transformation method to do the final linear transformation to get\\nthe probability yi of the current input character being labeled.\\nThe linear expression is:\\nyi = w2 × xi − b2 (8)\\nIn the above formula, w2 is the weight matrix and b2 is the\\ndeviation coefficient.\\nB. WORD SEGMENTATION PROCESSING BASED ON\\nBI-GRU-CRF HYBRID NETWORK\\nAlthough the BI-LSTM network model has achieved excellent results in the accuracy of English word segmentation processing, the internal structure of the model is complex and the\\ntime cost of processing data is high. Therefore, this paper uses\\nthe BI-GRU neural network to reduce the time cost under the\\npremise of ensuring that the accuracy of the word processing\\nis close to that of the BI-LSTM network. Combined with\\nthe CRF model, a word segmentation model based on the\\nBI-GRU-CRF', doc_id='dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', embedding=None, doc_hash='be96beb267949818af895c16b83ea79fb8e4f04e022b0f18a4c068c20e30900a', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 28259, 'end': 32076, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: '09328148-502b-4307-a672-f67d5921c7a7', <DocumentRelationship.NEXT: '3'>: '709e1cb1-a777-4b50-a657-8967d615c990'}), score=0.8355766782152038)], extra_info={'709e1cb1-a777-4b50-a657-8967d615c990': {'catgeory': 'NLP', 'filename': 'NLP'}, 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4': {'catgeory': 'NLP', 'filename': 'NLP'}})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b5a48ed-99d5-4d58-8567-6205c41f3237",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7872\n",
      "Running on public URL: https://614e018be982ba7936.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://614e018be982ba7936.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# inputs=gr.Textbox(lines=2, placeholder=\"Text Here...\"),\n",
    "\n",
    "def generate_output(abstract):\n",
    "    response = query_engine.query(abstract)\n",
    "    return response\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_output,\n",
    "    # inputs=inputs,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Text Here...\"),\n",
    "    outputs=\"text\",\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3d38352-1f7e-4bff-a19d-366d0a9caf35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "43dc7d17-47af-4ef0-9176-3037d1c47b24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "Running on public URL: https://441be546e7421f74ac.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://441be546e7421f74ac.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Initialize a variable to store the input\n",
    "stored_inputs = []\n",
    "\n",
    "def generate_output(abstract):\n",
    "    # Store the input value in the variable\n",
    "    stored_inputs.append(abstract) \n",
    "    # Perform other operations or computations using the input value\n",
    "    response = query_engine.query(abstract)\n",
    "    return response\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_output,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Abstract Here...\"),\n",
    "    outputs=gr.Textbox(label=\"Output\")\n",
    ")\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7650eefd-12bc-43ba-9af6-d2ef903a3fad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'709e1cb1-a777-4b50-a657-8967d615c990': {'catgeory': 'NLP',\n",
       "  'filename': 'NLP'},\n",
       " 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4': {'catgeory': 'NLP',\n",
       "  'filename': 'NLP'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db433380-e489-4698-b22e-8660b1fc6230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "Running on public URL: https://9773900d32bad80e48.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9773900d32bad80e48.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We introduce a new language representation model called BERT, which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial taskspecific architecture modifications.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art results on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize variables to store the inputs\n",
    "stored_title = []\n",
    "stored_text = []\n",
    "stored_response = []\n",
    "\n",
    "def get_categoy_list(response):   \n",
    "    category_list = []\n",
    "    for rp in response.extra_info:      \n",
    "        cat = response.extra_info[rp]['catgeory']\n",
    "        category_list.append(cat)\n",
    "        \n",
    "def get_sorted_list(category_list):  \n",
    "    # Get the count of each item\n",
    "    counts = Counter(category_list)\n",
    "    # Sort the items based on their counts in ascending order\n",
    "    sorted_counts = sorted(counts.items(), key=lambda x: x[1])\n",
    "    # Print the sorted counts\n",
    "    for item, count in sorted_counts:      \n",
    "        print(item, count)\n",
    "    return sorted_counts[0][0]\n",
    "\n",
    "def generate_output(input1, input2, input3):\n",
    "    # Store the input values in the respective variables\n",
    "    stored_title.append(input1)\n",
    "    stored_text.append(input3)  \n",
    "    # print(input2)\n",
    "    response = query_engine.query(input2) \n",
    "    # stored_response.append(response)\n",
    "    cat_list = get_category_list(response)\n",
    "    print(cat_list)\n",
    "    \n",
    "    category = get_sorted_list(cat_list)\n",
    "    return response\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=generate_output,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Title of the research paper\"),\n",
    "        gr.Textbox(label=\"Abstract of the paper\"),\n",
    "        gr.Textbox(label=\"Text\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Output\")\n",
    ")\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f37e1b08-1db5-4b43-88f0-b1754115b94d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Response(response='Empty Response', source_nodes=[NodeWithScore(node=Node(text='model, a word segmentation model based on the\\nBI-GRU-CRF hybrid network is proposed. This model can\\nnot only cost less time, but also retains the function of BILSTM to resolve long-distance dependencies, and also uses\\nthe CRF layer to efficiently consider the label information\\nbefore and after the sentence. The model structure diagram is\\nshown in Figure 6.\\nLet the newly introduced state transition matrix be A, and\\nthe output of the double-layer GRU neural network be matrix\\nP. Where Ai,j represents the weight transferred from label i to\\nlabel j in time series. If the value of Ai,j\\nis greater, it means\\nthat the probability of transfer from label i to label j is greater;\\nPi,j\\nis the input observation sequence. The i-th word is the\\nprobability of the j-th label. It can be concluded that the\\nprediction output of the labeled sequence y = (y1, y2,. . . , yn)\\ncorresponding to the observation sequence T = (t1, t2,\\n. . . , tn)\\ncan be expressed as:\\nt(T , y) =\\nXn\\ni=1\\n(A + Pi,y1\\n) (9)\\nC. SIMULATION EXPERIMENT AND ANALYSIS\\nTRAINING METHOD AND EXPERIMENTAL STEPS USING\\nTHE MODEL\\nIn the process of English word segmentation, what we have to\\nsolve is how to correctly label each word in the input text so\\nthat the required vocabulary can be identified according to the\\nlabel in the subsequent processing. This paper uses the sixposition tagging method to mark the text separately and uses\\nthe dropout method to avoid overfitting problems in neural\\nnetworks.\\nVOLUME 8, 2020 46341\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 6. Schematic diagram of the word segmentation structure of\\nBI-GRU-CRF hybrid network.\\nThe steps of BI-GRU-CRF-based English word segmentation processing method proposed in this paper are as follows:\\n(1) We input the question sentence into the word embedding layer, and perform word vector conversion through the\\nmethod of word2vec, so that each word is transformed into a\\nspace vector with a length of d;\\n(2) We use the two-way gate recurrent network to\\nautonomously learn the space vector obtained by the previous\\nconversion, obtain the positive and negative semantic features\\nin the vector, adjust the weight and deviation coefficients\\nin the network, and finally obtain the feature vector of the\\nproblem;\\n(3) Probability calculation is performed on the feature\\nvector through the Softmax layer to obtain the part-of-speech\\nprobability distribution of words;\\n(4) We input the part-of-speech probability distribution of\\nthe words into the conditional random field layer, perform\\nsequence labeling at the sentence level, and obtain the final\\nword segmentation processing result.\\nEXPERIMENTAL DATA AND PARAMETER SETTINGS\\nThe data used in the experiment are from three corpus\\ndatabases: SQuAD, Wiki Text, and NarrativeQA. During the\\nexperiment, this data set is divided into three categories:\\ntraining data set, development data set, and test data set. Since\\nthe SQuAD and Wiki Text datasets directly contain test sets,\\nthe test sets provided by them are directly adopted. 10% of the\\ntraining set is randomly selected as the development set, and\\nthe rest of the training set is used as the training set for this\\nexperiment. All data are pre-processed before training on the\\ninput data set. Idioms, consecutive English words or numbers\\nin the data set are replaced with ∗. In order to ensure the\\nFIGURE 7. Comparison results of different models on the SQuAD, Wiki\\nText, and NarrativeQA test sets.\\nreliability of the experimental comparison, the parameters of\\nthe experimental environment and the neural network are set\\nuniformly in this paper. The same parameters are used for\\nall network models. The specific parameter data is shown\\nin Table', doc_id='709e1cb1-a777-4b50-a657-8967d615c990', embedding=None, doc_hash='ff35b589f2f19e56d3717ca70e1505d895943407d7ff01ad549218bcfb11ce72', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 32125, 'end': 35839, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', <DocumentRelationship.NEXT: '3'>: 'e47306bd-0a13-472b-b254-a0872d3e2fa7'}), score=0.8609934078429707), NodeWithScore(node=Node(text='valid features,\\nand the feature discrimination ability obtained when different\\nmodalities are missing is only slightly lost. At the same time,\\nit was found that using unsupervised data for model training\\ncan improve the discriminative ability of the model to extract\\nfeatures. The decrease in classification accuracy when the\\nmodal is missing is more moderate and the accuracy decline\\nis smaller than using only labeled data. The accuracy of the\\npairwise classification of the dataset in different layers is\\nshown in Figure 5.\\nIII. ENGLISH WORD SEGMENTATION PROCESSING\\nBASED ON BI-GRU-CRF MIXED NETWORK\\nA. SEGMENTATION PROCESSING FRAMEWORK BASED\\nON NEURAL NETWORK\\nThe character-based sequence labeling task can be similarly\\nregarded as an English word segmentation task. Specifically,\\n46340 VOLUME 8, 2020\\nD. Wang et al.: Feature Extraction and Analysis of NLP for Deep Learning English Language\\nFIGURE 5. Classification accuracy of the dataset at different layers of the\\nproposed model.\\ngiven a mark for each character in the input sentence, during\\nthe word segmentation processing phase, these marks can be\\nused to identify the position of the word in the sentence.\\nIn the research of word segmentation processing based on\\ndeep learning methods, the commonly used annotation set\\nfor researchers is the four-position (B, M, E, S) annotation\\nset, where B represents the beginning of the sentence and M\\nrepresents the middle, E is the ending participle, and S is the\\nparticiple composed of individual words. The other tag set is a\\nsix-position (B, B1, B2, M, E, S) tag set, where B1 and B2 are\\nused to finely divide characters and more effectively label\\nthe position of the characters in the sentence. English word\\nsegmentation framework based on neural network generally\\nincludes three parts: (1) the character embedding layer, that\\nis, the input English characters are converted into a vector\\nmatrix through Word Embedding; (2) a conversion layer with\\nmultiple neural networks; (3) a labeling discrimination layer,\\nwhich is used for subsequent discrimination, and each word\\nmarks its own tag information.\\nWe open a context window for each character in the\\nsentence, which is a commonly used method of character\\nmarking, and then use other character characteristics in the\\nwindow to determine the category mark of the character.\\nGiven a sentence c (n) of input length n, it sets the size of the\\nwindow to w, and the starting character c (1) slides through\\nthe window to the end character c (n). The specific steps of\\nneural network English word segmentation are as follows:\\n(1) Let zi be the linear conversion result from the input\\nlayer to the hidden layer, and its expression is:\\nzi = w1 × xi + b1 (6)\\nIn the above formula, w1 is the weight matrix and b1 is the\\ndeviation coefficient.\\n(2) We pass the linear transformation result through the\\nelement-level activation function σ to obtain the hidden layer\\nfunction. The expression is:\\nhi = σ(zi) (7)\\n(3) Using the given set of labels, we use the linear transformation method to do the final linear transformation to get\\nthe probability yi of the current input character being labeled.\\nThe linear expression is:\\nyi = w2 × xi − b2 (8)\\nIn the above formula, w2 is the weight matrix and b2 is the\\ndeviation coefficient.\\nB. WORD SEGMENTATION PROCESSING BASED ON\\nBI-GRU-CRF HYBRID NETWORK\\nAlthough the BI-LSTM network model has achieved excellent results in the accuracy of English word segmentation processing, the internal structure of the model is complex and the\\ntime cost of processing data is high. Therefore, this paper uses\\nthe BI-GRU neural network to reduce the time cost under the\\npremise of ensuring that the accuracy of the word processing\\nis close to that of the BI-LSTM network. Combined with\\nthe CRF model, a word segmentation model based on the\\nBI-GRU-CRF', doc_id='dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4', embedding=None, doc_hash='be96beb267949818af895c16b83ea79fb8e4f04e022b0f18a4c068c20e30900a', extra_info={'catgeory': 'NLP', 'filename': 'NLP'}, node_info={'start': 28259, 'end': 32076, '_node_type': <NodeType.TEXT: '1'>}, relationships={<DocumentRelationship.SOURCE: '1'>: '963674de-2cdd-490c-b97a-0f32d2bdae89', <DocumentRelationship.PREVIOUS: '2'>: '09328148-502b-4307-a672-f67d5921c7a7', <DocumentRelationship.NEXT: '3'>: '709e1cb1-a777-4b50-a657-8967d615c990'}), score=0.8354277013046536)], extra_info={'709e1cb1-a777-4b50-a657-8967d615c990': {'catgeory': 'NLP', 'filename': 'NLP'}, 'dd6539b8-fa19-4bf6-81d1-cdb821b2e7c4': {'catgeory': 'NLP', 'filename': 'NLP'}})]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cdb85a40-eb65-44ba-854f-5889d2f155d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_list = []\n",
    "for rp in response.extra_info:\n",
    "    cat = response.extra_info[rp]['catgeory']\n",
    "    category_list.append(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "111bf3c6-0e31-41ab-8ba8-0d1e811f0d38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP', 'NLP']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32b362e9-e15a-4d31-a885-6fa83c66ede6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Get the count of each item\n",
    "counts = Counter(category_list)\n",
    "\n",
    "# Sort the items based on their counts in ascending order\n",
    "sorted_counts = sorted(counts.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the sorted counts\n",
    "for item, count in sorted_counts:\n",
    "    print(item, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f967eb5-173f-459b-b06c-bfb1de16f012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', 2)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9ffc7947-06f0-41a0-9129-ad031ddcd9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database = sorted_counts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "469a3352-72ab-4385-b5b7-32c1dd8f72b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database it belongs to is NLP\n"
     ]
    }
   ],
   "source": [
    "print(f\"The database it belongs to is {database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0f47c71f-6b69-4a56-a6ce-69dd6e8354c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'51522920a0934fdbb4567e7640f2e7d3': 'CV',\n",
       " 'd941c28a34934793b61a509ad5c2360c': 'NLP'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "88d695bb-54be-4baf-a9cb-4458f5b11d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "database_id = ''\n",
    "for key, value in my_dict.items():\n",
    "    if value == database:\n",
    "        database_id = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21747110-4a1a-4e3a-bbc1-c364b058dc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Language model pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand Brockett, 2005), which aim to predict the relationships between sentences by analyzing them\\nholistically, as well as token-level tasks such as\\nnamed entity recognition and question answering,\\nwhere models are required to produce fine-grained\\noutput at the token level (Tjong Kim Sang and\\nDe Meulder, 2003; Rajpurkar et al., 2016).\\nThere are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning. The\\nfeature-based approach, such as ELMo (Peters\\net al., 2018a), uses task-specific architectures that\\ninclude the pre-trained representations as additional features. The fine-tuning approach, such as\\nthe Generative Pre-trained Transformer (OpenAI\\nGPT) (Radford et al., 2018), introduces minimal\\ntask-specific parameters, and is trained on the\\ndownstream tasks by simply fine-tuning all pretrained parameters. The two approaches share the\\nsame objective function during pre-training, where\\nthey use unidirectional language models to learn\\ngeneral language representations.\\nWe argue that current techniques restrict the\\npower of the pre-trained representations, especially for the fine-tuning approaches. The major limitation is that standard language models are\\nunidirectional, and this limits the choice of architectures that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani et al., 2017). Such restrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when applying finetuning based approaches to token-level tasks such\\nas question answering, where it is crucial to incorporate context from both directions.\\nIn this paper, we improve the fine-tuning based\\napproaches by proposing BERT: Bidirectional\\nEncoder Representations from Transformers.\\nBERT alleviates the previously mentioned unidirectionality constraint by using a “masked language model” (MLM) pre-training objective, inspired by the Cloze task (Taylor, 1953). The\\nmasked language model randomly masks some of\\nthe tokens from the input, and the objective is to\\npredict the origin']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40545622-df8c-4c5d-9f93-2aef39a53335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d941c28a34934793b61a509ad5c2360c'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "06084081-e8d3-4a42-a140-62581bd692c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Authorization\": \"Bearer \" + token,\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Notion-Version\": \"2022-06-28\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a48105c-0da4-4efe-8ca7-69cd74c96396",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stored_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "31bad69e-0aad-4063-97cd-52fb083a83b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createPage(databaseId, headers, stored_text, stored_title):\n",
    "\n",
    "    createUrl = 'https://api.notion.com/v1/pages'\n",
    "\n",
    "    newPageData = {\n",
    "        \"parent\": {\"database_id\": databaseId },\n",
    "         \"properties\": {\n",
    "            \"title\": {\n",
    "              \"title\":[\n",
    "                {\n",
    "                  \"text\": {\n",
    "                    \"content\": stored_title[0] \n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "           }\n",
    "        },\n",
    "        # \"children\":[],\n",
    "         \"children\": [\n",
    "        {\n",
    "            \"object\": \"block\",\n",
    "            \"type\": \"paragraph\",\n",
    "            \"paragraph\": {\n",
    "                \"rich_text\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": {\n",
    "                            \"content\": stored_text[0][:100]\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "     }\n",
    "    \n",
    "    # notion.pages.create(parent={\"database_id\": database_id}, properties=new_page,children=new_child)\n",
    "\n",
    "\n",
    "    \n",
    "    data = json.dumps(newPageData)\n",
    "    print(data)\n",
    "    res = requests.request(\"POST\", createUrl, headers=headers, data=data)\n",
    "\n",
    "    print(res.status_code)\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "70d9dcfa-4e5d-4742-ab1f-b86c16881c99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"parent\": {\"database_id\": \"d941c28a34934793b61a509ad5c2360c\"}, \"properties\": {\"title\": {\"title\": [{\"text\": {\"content\": \"BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\"}}]}}, \"children\": [{\"object\": \"block\", \"type\": \"paragraph\", \"paragraph\": {\"rich_text\": [{\"type\": \"text\", \"text\": {\"content\": \"Language model pre-training has been shown to\\nbe effective for improving many natural language\\nproce\"}}]}}]}\n",
      "200\n",
      "{\"object\":\"page\",\"id\":\"b54d1c20-8fe6-42d0-bbd8-2c8cb820f0b2\",\"created_time\":\"2023-06-18T20:43:00.000Z\",\"last_edited_time\":\"2023-06-18T20:43:00.000Z\",\"created_by\":{\"object\":\"user\",\"id\":\"6006384d-2bce-405d-812e-0e8b7a1c90fc\"},\"last_edited_by\":{\"object\":\"user\",\"id\":\"6006384d-2bce-405d-812e-0e8b7a1c90fc\"},\"cover\":null,\"icon\":null,\"parent\":{\"type\":\"database_id\",\"database_id\":\"d941c28a-3493-4793-b61a-509ad5c2360c\"},\"archived\":false,\"properties\":{\"Text\":{\"id\":\"TN~L\",\"type\":\"multi_select\",\"multi_select\":[]},\"Name\":{\"id\":\"title\",\"type\":\"title\",\"title\":[{\"type\":\"text\",\"text\":{\"content\":\"BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\",\"link\":null},\"annotations\":{\"bold\":false,\"italic\":false,\"strikethrough\":false,\"underline\":false,\"code\":false,\"color\":\"default\"},\"plain_text\":\"BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\",\"href\":null}]}},\"url\":\"https://www.notion.so/BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-b54d1c208fe642d0bbd82c8cb820f0b2\",\"public_url\":null}\n"
     ]
    }
   ],
   "source": [
    "createPage(database_id, headers, stored_text, stored_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e546e67-cf4a-4b8e-8500-960c9a9c19e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
